{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda create -n sss python=3.8 -y && conda activate sss\n",
    "# conda install -y -c bioconda gffutils jupyter tqdm cyvcf2 pathlib2 pandarallel pysam liftover pybedtools\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from Bio.Seq import Seq\n",
    "# from liftover import get_lifter\n",
    "from pathlib2 import Path\n",
    "from pandarallel import pandarallel\n",
    "from tqdm import tqdm\n",
    "import gffutils\n",
    "import pysam\n",
    "from cyvcf2 import VCF\n",
    "\n",
    "### Logging setup\n",
    "from logging import getLogger, config\n",
    "import yaml\n",
    "parent_directory = os.path.dirname(os.path.dirname('__file__'))\n",
    "config_path: str = os.path.join(parent_directory, '../../../config/logging.yaml')\n",
    "with open(config_path, 'r') as f:\n",
    "    config.dictConfig(yaml.safe_load(f))\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "########   Initialize and setup pandas methods   ########\n",
    "os.environ['JOBLIB_TEMP_FOLDER'] = '/tmp' \n",
    "pandarallel.initialize(nb_workers=5, progress_bar=True, verbose=1, use_memory_fs=False) \n",
    "tqdm.pandas()\n",
    "\n",
    "import sys\n",
    "try: \n",
    "    __file__\n",
    "    sys.path.append(os.path.join(os.path.dirname('__file__')))\n",
    "except NameError:\n",
    "    Path().resolve()\n",
    "    sys.path.append(os.path.join(Path().resolve(), '../../../'))\n",
    "\n",
    "from libs import utils, preprocess, variantfilter, posparser, splaiparser\n",
    "# from libs import predeffect, scoring\n",
    "from libs import anno_spliceai, anno_clinvar\n",
    "from libs.deco import print_filtering_count\n",
    "# from libs import predeffect\n",
    "from libs.scoring import Scoring\n",
    "from libs import predeffect\n",
    "\n",
    "\n",
    "gencode_gff = '../../../Resources/05_GENCODE_v43lift37/gencode.v43lift37.annotation.sort.gff3.gz'\n",
    "\n",
    "try:\n",
    "    db_anno_gencode = '../../../Resources/06_gffutilsdb/gencode.v43lift37.annotation.gtf.db'\n",
    "    db_anno_intron = '../../../Resources/06_gffutilsdb/gencode.v43lift37.annotation.intron.gtf.db'\n",
    "    db = gffutils.FeatureDB(db_anno_gencode)\n",
    "    db_intron = gffutils.FeatureDB(db_anno_intron)\n",
    "except ValueError:\n",
    "    db_anno_gencode = '/resources/DBs/gencode.v43lift37.annotation.gtf.db'\n",
    "    db_anno_intron = '/resources/DBs/gencode.v43lift37.annotation.intron.gtf.db'\n",
    "    db = gffutils.FeatureDB(db_anno_gencode)\n",
    "    db_intron = gffutils.FeatureDB(db_anno_intron)\n",
    "\n",
    "## Thresholds configuration\n",
    "thresholds_SpliceAI_parser: dict = {\n",
    "    'TH_min_sALDL': 0.02, 'TH_max_sALDL': 0.2, \n",
    "    'TH_min_sAGDG': 0.01, 'TH_max_sAGDG': 0.05,\n",
    "    'TH_min_GExon': 25, 'TH_max_GExon': 500,\n",
    "    'TH_sAG': 0.2, 'TH_sDG': 0.2\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 前処理．SQLから抽出したallmut.csvを編集する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mut_default_colnames: list = [\n",
    "    \"disase\", \"gene\", \"chrom\", \"genename\", \"gdbid\", \"omimid\", \"amino\", \n",
    "    \"deletion\", \"insertion\", \"codon\", \"codonAff\", \"descr\", \"refseq\", \"hgvs\", \n",
    "    \"hgvsAll\", \"dbsnp\", \"chromosome\", \"startCoord\", \"endCoord\", \n",
    "    \"expected_inheritance\", \"gnomad_AC\", \"gnomad_AF\", \"gnomad_AN\", \"tag\", \n",
    "    \"dmsupport\", \"rankscore\", \"mutype\", \"author\", \"title\", \"fullname\", \n",
    "    \"allname\", \"vol\", \"page\", \"year\", \"pmid\", \"pmidAll\", \"reftag\", \"comments\", \n",
    "    \"acc_num\", \"new_date\", \"base\", \"clinvarID\", \"clinvar_clnsig\"\n",
    "]\n",
    "\n",
    "allmut: pd.DataFrame = pd.read_csv(\n",
    "    'allmut.csv', sep=';', encoding='cp1252', names=all_mut_default_colnames, \n",
    "    skiprows=1,low_memory=False)\n",
    "\n",
    "\n",
    "allmut: pd.DataFrame = pd.read_csv(\n",
    "    'allmut.csv', sep=';', encoding='cp1252', names=all_mut_default_colnames, \n",
    "    skiprows=1,low_memory=False)\n",
    "\n",
    "allmut = allmut[\n",
    "    [\"gene\", \"genename\", \"mutype\", \"clinvar_clnsig\", \"tag\",\n",
    "     \"refseq\", \"hgvs\", \"hgvsAll\", \"chromosome\", \"startCoord\", \"endCoord\", \n",
    "     \"amino\", \"deletion\", \"insertion\", \"expected_inheritance\", \"gnomad_AF\"]]\n",
    "\n",
    "# Drop non-numeric values in 'startCoord'\n",
    "allmut = allmut.dropna(subset=['startCoord'])\n",
    "\n",
    "# Drop duplicates in 'chrom', 'startCoord', and 'endCoord'\n",
    "allmut = allmut.drop_duplicates(subset=['chromosome', 'startCoord', 'endCoord'])\n",
    "\n",
    "# Extract tag == \"DM\" from allmut\n",
    "allmut_dm = allmut[allmut.tag == \"DM\"].copy()\n",
    "print(f\"A total of {len(allmut_dm)} DM mutations are found in allmut.\")\n",
    "\n",
    "allmut_dm['startCoord'] = allmut_dm['startCoord'].astype(int)\n",
    "allmut_dm = allmut_dm.rename(columns={'chromosome': 'CHROM', 'startCoord': 'POS_hg38'})\n",
    "\n",
    "# Fillna with empty string in \"gnomad_AF\" colmun in allmut_dm\n",
    "# Extratct MAF 0 from allmut_dm\n",
    "allmut_dm['gnomad_AF'].fillna(0, inplace=True)\n",
    "allmut_dm_maf0 = allmut_dm[allmut_dm['gnomad_AF'] == 0].copy()\n",
    "print(f\"A total of {len(allmut_dm_maf0)} DM mutations are found in allmut with MAF 0.\")\n",
    "\n",
    "# Extract non-deletion or non-insertion from allmut_dm\n",
    "allmut_dm_maf0_snv = allmut_dm_maf0[(allmut_dm_maf0['deletion'].isnull()) & (allmut_dm_maf0['insertion'].isnull())]\n",
    "print(f\"A total of {len(allmut_dm_maf0_snv)} DM mutations are found in allmut with MAF 0 and non-deletion or non-insertion.\")\n",
    "\n",
    "# Extract the mutation type from the mutype column\n",
    "splice_mutations = allmut_dm_maf0_snv[allmut_dm_maf0_snv[\"mutype\"].str.contains(\"splice\")].copy()\n",
    "non_splice_mutations = allmut_dm_maf0_snv[~allmut_dm_maf0_snv[\"mutype\"].str.contains(\"splice\")]\n",
    "print(f\"Splicing_DM: {len(splice_mutations)}, Non-splicing_DM: {len(non_splice_mutations)}\")\n",
    "\n",
    "# Convert startCoord to hg19\n",
    "from liftover import get_lifter\n",
    "\n",
    "def _liftover_to_hg19(chrom, pos):\n",
    "    converter = get_lifter('hg38', 'hg19')\n",
    "    result = converter.query(chrom, pos)\n",
    "    if result:\n",
    "        return result[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def anno_hg19_pos(row):\n",
    "    converted = _liftover_to_hg19(row['CHROM'], row['POS_hg38'])\n",
    "    if converted:\n",
    "        return converted[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "allmut_dm_maf0_snv['POS_hg19'] = allmut_dm_maf0_snv.parallel_apply(anno_hg19_pos, axis=1)\n",
    "# allmut_dm_maf0_snv.to_pickle('allmut_dm_maf0_snv_liftover.pkl', mode='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liftover処理後（ここから解析すればOK）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256603\n",
      "126859\n",
      "Splicing: 18432, Non-splicing: 107021, total: 125453\n"
     ]
    }
   ],
   "source": [
    "# Loading allmut variants from pickle\n",
    "allmut_dm_maf0_snv_hg19 = pd.read_pickle('allmut_dm_maf0_snv_liftover.pkl')\n",
    "\n",
    "# Rename POS_hg19 to POS\n",
    "allmut_dm_maf0_snv_hg19.rename(columns={'POS_hg19': 'POS'}, inplace=True)\n",
    "\n",
    "# Drop unknown positions in 'POS' column and assign integer type\n",
    "allmut_dm_maf0_snv_hg19.dropna(subset=['POS'], inplace=True)\n",
    "allmut_dm_maf0_snv_hg19 = allmut_dm_maf0_snv_hg19.astype({'POS': int})\n",
    "\n",
    "# Change object name to allmut\n",
    "allmut = allmut_dm_maf0_snv_hg19\n",
    "\n",
    "# Generate ID column\n",
    "allmut['ID'] = allmut['CHROM'].astype(str) + '-' + allmut['POS'].astype(str) + '-' + allmut['hgvs']\n",
    "\n",
    "# Extract useful columns\n",
    "allmut = allmut[['ID', 'mutype', 'clinvar_clnsig', 'tag', 'deletion', 'insertion', 'expected_inheritance', 'gnomad_AF']]\n",
    "\n",
    "# Load VCF file annoteted by analysis pipeline\n",
    "df = pd.read_pickle('splai_vep_vcfs/hgmd_dm/allchr.DM.splai.vep.nondel.enst.prescore.hgnconly.v2.pkl')\n",
    "df['HGVSc'] = df['HGVSc'].str.replace('c.', '')\n",
    "df['ID'] = df['CHROM'].astype(str) + '-' + df['POS'].astype(str) + '-' + df['HGVSc']\n",
    "\n",
    "# merge df and allmut on 'ID' column with inner join\n",
    "print(len(df))\n",
    "df = pd.merge(df, allmut, on='ID', how='inner')\n",
    "print(len(df))\n",
    "\n",
    "exclude_csq = {\n",
    "    '3_prime_UTR_variant', '5_prime_UTR_variant', 'mature_miRNA_variant',\n",
    "    'mature_miRNA_variant', 'downstream_gene_variant', 'upstream_gene_variant'\n",
    "}\n",
    "\n",
    "def is_orf_variants(row):\n",
    "    csqs: list = row['Consequence'].split('&')\n",
    "    if set(csqs).isdisjoint(exclude_csq):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "df['is_ORF'] = df.apply(is_orf_variants, axis=1)\n",
    "df = df[df['is_ORF']]\n",
    "\n",
    "# df_spl contains splicing mutations (splice, canonical-splice, exonic-splice)\n",
    "df_spl = df[df['mutype'].str.contains('splice')].copy()\n",
    "\n",
    "# df_non_spl contains non-splicing mutations (missense, nonsense, synonymous)\n",
    "df_non_spl = df[df['mutype'].str.contains('missense|nonsense|synonymous')].copy()\n",
    "\n",
    "print(f\"Splicing: {len(df_spl)}, Non-splicing: {len(df_non_spl)}, total: {len(df_spl) + len(df_non_spl)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotating the label and variant_id (CHROM-POS-REF_ALT)\n",
    "# When mutype is splice, the label is 1, otherwise 0\n",
    "df_gnomad = pd.read_pickle('splai_vep_vcfs/gnomadv211/allchr.gnomad.splai.vep.vcf.enst.prescore.hgnconly.v2.pkl')\n",
    "df_gnomad['is_ORF'] = df_gnomad.apply(is_orf_variants, axis=1)\n",
    "df_gnomad = df_gnomad[df_gnomad['is_ORF']]\n",
    "\n",
    "df_spl['LABEL'] = 1\n",
    "df_non_spl['LABEL'] = 0\n",
    "df_gnomad['LABEL'] = 0\n",
    "\n",
    "df_spl['variant_id'] = df_spl['CHROM'].astype(str) + '-' + df_spl['POS'].astype(str) + '-' + df_spl['REF'] + '-' + df_spl['ALT']\n",
    "df_non_spl['variant_id'] = df_non_spl['CHROM'].astype(str) + '-' + df_non_spl['POS'].astype(str) + '-' + df_non_spl['REF'] + '-' + df_non_spl['ALT']\n",
    "df_gnomad['variant_id'] = df_gnomad['CHROM'].astype(str) + '-' + df_gnomad['POS'].astype(str) + '-' + df_gnomad['REF'] + '-' + df_gnomad['ALT']\n",
    "\n",
    "# Create a dataframe tp\n",
    "tp = df_spl.copy()\n",
    "\n",
    "# Concatenate df_gnomad and df_non_spl\n",
    "tn = pd.concat([df_gnomad, df_non_spl], ignore_index=True)\n",
    "tn = tn.drop_duplicates(subset=['variant_id'], keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 7 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "from pandarallel import pandarallel\n",
    "from tqdm import tqdm\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "########   Initialize and setup pandas methods   ########\n",
    "pandarallel.initialize(nb_workers=os.cpu_count()-1, progress_bar=False, \n",
    "                       verbose=2, use_memory_fs=False) \n",
    "os.environ['JOBLIB_TEMP_FOLDER'] = '/tmp' \n",
    "\n",
    "try: \n",
    "    __file__\n",
    "    sys.path.append(os.path.join(os.path.dirname('__file__')))\n",
    "except NameError:\n",
    "    Path().resolve()\n",
    "    sys.path.append(os.path.join(Path().resolve(), '../../'))\n",
    "\n",
    "from libs.scoring import Scoring\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total solutions found: 4850\n"
     ]
    }
   ],
   "source": [
    "def specificity_sensitivity_plotly(data):\n",
    "    thresholds = np.arange(0, 11, 1)\n",
    "    results = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        tp = data[(data['PriorityScore'] >= threshold) & (data['LABEL'] == 1)].shape[0]\n",
    "        fn = data[(data['PriorityScore'] < threshold) & (data['LABEL'] == 1)].shape[0]\n",
    "        tn = data[(data['PriorityScore'] < threshold) & (data['LABEL'] == 0)].shape[0]\n",
    "        fp = data[(data['PriorityScore'] >= threshold) & (data['LABEL'] == 0)].shape[0]\n",
    "        specificity = tn / (tn + fp) if (tn + fp) else 0\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) else 0\n",
    "        # print(f\"Threshold: {threshold}, TP: {tp}, FN: {fn}, TN: {tn}, FP: {fp}\")\n",
    "        # print(f\"Threshold: {threshold}, Specificity: {specificity:.6f}, Sensitivity: {sensitivity:.6f}\")\n",
    "        results.append({'Threshold': threshold, 'Metric': 'Specificity', 'Value': specificity})\n",
    "        results.append({'Threshold': threshold, 'Metric': 'Sensitivity', 'Value': sensitivity})\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "def plot_sensitivity_specificity_plotly(\n",
    "        results_df: pd.DataFrame, w: int, h: int):\n",
    "    # Separate the dataframes for specificity and sensitivity\n",
    "    specificity_df = results_df[results_df['Metric'] == 'Specificity']\n",
    "    sensitivity_df = results_df[results_df['Metric'] == 'Sensitivity']\n",
    "\n",
    "    # Plotly Graph Objectsを使用してプロット\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # 特異性\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=specificity_df['Threshold'],\n",
    "        y=specificity_df['Value'],\n",
    "        marker=dict(color='#665990'),\n",
    "        mode='lines+markers',\n",
    "        name='Specificity',\n",
    "        text=[f'Threshold: {th}, Specificity: {val:.3f}' for th, val in zip(specificity_df['Threshold'], specificity_df['Value'])],\n",
    "        hoverinfo='text'\n",
    "    ))\n",
    "    \n",
    "    # 感度\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=sensitivity_df['Threshold'],\n",
    "        y=sensitivity_df['Value'],\n",
    "        marker=dict(color='#F8ACAC'),\n",
    "        mode='lines+markers',\n",
    "        name='Sensitivity',\n",
    "        text=[f'Threshold: {th}, Sensitivity: {val:.3f}' for th, val in zip(sensitivity_df['Threshold'], sensitivity_df['Value'])],\n",
    "        hoverinfo='text'\n",
    "    ))\n",
    "\n",
    "    # Y軸のフォーマット設定\n",
    "    fig.update_yaxes(tickformat=\".1f\")\n",
    "\n",
    "    # グラフのレイアウト設定\n",
    "    fig.update_layout(title='Sensitivity and Specificity for each threshold',\n",
    "                      xaxis_title='Threshold',\n",
    "                      yaxis_title='Sensitivity/Specificity',\n",
    "                      plot_bgcolor='rgba(243, 243, 243, 1)',\n",
    "                      paper_bgcolor='rgba(243, 243, 243, 0)',\n",
    "                      font=dict(family=\"Arial, sans-serif\", size=12, color=\"black\"),\n",
    "                      legend=dict(y=0.075, x=0.75, xanchor='right', yanchor='bottom', \n",
    "                              bgcolor='rgba(243, 243, 243, 1)',\n",
    "                              font=dict(family=\"Arial, sans-serif\", size=12, color=\"black\"))\n",
    "                              )\n",
    "\n",
    "    # グラフサイズの調整\n",
    "    fig.update_layout(width=w, height=h)\n",
    "    fig.write_html(\"sensitivity_specificity_plot.html\")\n",
    "\n",
    "    # fig.show()\n",
    "    return fig\n",
    "\n",
    "def plot_sensitivity_specificity_plotly_without_legened(results_df):\n",
    "    # Separate the dataframes for specificity and sensitivity\n",
    "    specificity_df = results_df[results_df['Metric'] == 'Specificity']\n",
    "    sensitivity_df = results_df[results_df['Metric'] == 'Sensitivity']\n",
    "\n",
    "    # Plotly Graph Objectsを使用してプロット\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # 特異性\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=specificity_df['Threshold'],\n",
    "        y=specificity_df['Value'],\n",
    "        marker=dict(color='green'),\n",
    "        mode='lines+markers',\n",
    "        name='Specificity',\n",
    "        text=[f'Threshold: {th}, Specificity: {val:.8f}' for th, val in zip(specificity_df['Threshold'], specificity_df['Value'])],\n",
    "        hoverinfo='text',\n",
    "        showlegend=False \n",
    "    ))\n",
    "    \n",
    "    # 感度\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=sensitivity_df['Threshold'],\n",
    "        y=sensitivity_df['Value'],\n",
    "        marker=dict(color='orange'),\n",
    "        mode='lines+markers',\n",
    "        name='Sensitivity',\n",
    "        text=[f'Threshold: {th}, Sensitivity: {val:.8f}' for th, val in zip(sensitivity_df['Threshold'], sensitivity_df['Value'])],\n",
    "        hoverinfo='text',\n",
    "        showlegend=False\n",
    "    ))\n",
    "\n",
    "    # Y軸のフォーマット設定\n",
    "    fig.update_yaxes(tickformat=\".1f\")\n",
    "\n",
    "    # グラフのレイアウト設定\n",
    "    fig.update_layout(title='Sensitivity and Specificity for each threshold',\n",
    "                      xaxis_title='Threshold',\n",
    "                      yaxis_title='Sensitivity/Specificity',\n",
    "                      plot_bgcolor='rgba(243, 243, 243, 1)',\n",
    "                      paper_bgcolor='rgba(243, 243, 243, 0)',\n",
    "                      font=dict(family=\"Arial, sans-serif\", size=12, color=\"black\"),\n",
    "                      legend=dict(y=0.075, x=0.75, xanchor='right', yanchor='bottom', \n",
    "                              bgcolor='rgba(243, 243, 243, 1)',\n",
    "                              font=dict(family=\"Arial, sans-serif\", size=12, color=\"black\"))\n",
    "                              )\n",
    "\n",
    "    # グラフサイズの調整\n",
    "    fig.update_layout(width=600, height=600)\n",
    "    fig.write_html(\"sensitivity_specificity_plot.html\")\n",
    "\n",
    "    # fig.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Code below is adapted from Netflix's VMAF and BesenbacherLab's ROC-utils\n",
    "# https://github.com/Netflix/vmaf/\n",
    "# https://github.com/BesenbacherLab/ROC-utils\n",
    "# Modifications: np.float -> np.float64\n",
    "\n",
    "def compute_midrank(x):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float64)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = 0.5*(i + j - 1)\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float64)\n",
    "    # Note(kazeevn) +1 is due to Python using 0-based indexing\n",
    "    # instead of 1-based in the AUC formula in the paper\n",
    "    T2[J] = T + 1\n",
    "    return T2\n",
    "\n",
    "\n",
    "def fastDeLong(predictions_sorted_transposed, label_1_count):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Operating Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float64)\n",
    "    ty = np.empty([k, n], dtype=np.float64)\n",
    "    tz = np.empty([k, m + n], dtype=np.float64)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank(positive_examples[r, :])\n",
    "        ty[r, :] = compute_midrank(negative_examples[r, :])\n",
    "        tz[r, :] = compute_midrank(predictions_sorted_transposed[r, :])\n",
    "    aucs = tz[:, :m].sum(axis=1) / m / n - float(m + 1.0) / 2.0 / n\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / n\n",
    "    v10 = 1.0 - (tz[:, m:] - ty[:, :]) / m\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def calc_pvalue(aucs, sigma):\n",
    "    \"\"\"Computes log(10) of p-values.\n",
    "    Args:\n",
    "       aucs: 1D array of AUCs\n",
    "       sigma: AUC DeLong covariances\n",
    "    Returns:\n",
    "       log10(pvalue)\n",
    "    \"\"\"\n",
    "    l = np.array([[1, -1]])\n",
    "    z = np.abs(np.diff(aucs)) / np.sqrt(np.dot(np.dot(l, sigma), l.T))\n",
    "    return np.log10(2) + scipy.stats.norm.logsf(z, loc=0, scale=1) / np.log(10)\n",
    "\n",
    "\n",
    "def compute_ground_truth_statistics(ground_truth):\n",
    "    assert np.array_equal(np.unique(ground_truth), [0, 1])\n",
    "    order = (-ground_truth).argsort()\n",
    "    label_1_count = int(ground_truth.sum())\n",
    "    return order, label_1_count\n",
    "\n",
    "\n",
    "def delong_roc_variance(ground_truth, predictions):\n",
    "    \"\"\"\n",
    "    Computes ROC AUC variance for a single set of predictions\n",
    "    Args:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions: np.array of floats of the probability of being class 1\n",
    "    \"\"\"\n",
    "    order, label_1_count = compute_ground_truth_statistics(ground_truth)\n",
    "    predictions_sorted_transposed = predictions[np.newaxis, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count)\n",
    "    assert len(aucs) == 1, \"There is a bug in the code, please forward this to the developers\"\n",
    "    return aucs[0], delongcov\n",
    "\n",
    "\n",
    "def delong_roc_test(ground_truth, predictions_one, predictions_two):\n",
    "    \"\"\"\n",
    "    Computes log(p-value) for hypothesis that two ROC AUCs are different\n",
    "    Args:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions_one: predictions of the first model,\n",
    "          np.array of floats of the probability of being class 1\n",
    "       predictions_two: predictions of the second model,\n",
    "          np.array of floats of the probability of being class 1\n",
    "    \"\"\"\n",
    "    order, label_1_count = compute_ground_truth_statistics(ground_truth)\n",
    "    predictions_sorted_transposed = np.vstack((predictions_one, predictions_two))[:, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count)\n",
    "    return calc_pvalue(aucs, delongcov)\n",
    "\n",
    "# Calculate AUC confidence interval (95%)\n",
    "def compute_auc_confidence_interval(auc, var, confidence_level=0.95):\n",
    "    alpha = 1 - confidence_level\n",
    "    z_score = scipy.stats.norm.ppf(1 - alpha/2)  # 2-tailed z score\n",
    "    se = np.sqrt(var)  # Calculate SE from variance\n",
    "    lower_bound = auc - z_score * se\n",
    "    upper_bound = auc + z_score * se\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "\n",
    "from ortools.sat.python import cp_model\n",
    "\n",
    "class SolutionCollector(cp_model.CpSolverSolutionCallback):\n",
    "    def __init__(self, variables):\n",
    "        cp_model.CpSolverSolutionCallback.__init__(self)\n",
    "        self.__variables = variables\n",
    "        self.__solutions = []\n",
    "\n",
    "    def OnSolutionCallback(self):\n",
    "        solution = {v.Name(): self.Value(v) for v in self.__variables}\n",
    "        self.__solutions.append(solution)\n",
    "\n",
    "    def GetAllSolutions(self):\n",
    "        return self.__solutions\n",
    "\n",
    "def find_all_solutions():\n",
    "    # モデルを初期化\n",
    "    model = cp_model.CpModel()\n",
    "\n",
    "    # 変数の定義\n",
    "    s = {i: model.NewIntVar(-10, 10, f's{i}') for i in range(1, 15)}\n",
    "\n",
    "    # 制約の追加\n",
    "    model.Add(s[3] == 0)                    #\n",
    "    model.Add(s[1] + s[10] + s[14] == 9)    #\n",
    "    model.Add(s[3] + s[11] + s[12] == 0)    #\n",
    "    # model.Add(s[1] + s[7] < 9)\n",
    "    # model.Add(s[10] >= 3)\n",
    "    # model.Add(s[10] <= 7)\n",
    "    \n",
    "    # Knowledge-based\n",
    "    # Absolutes\n",
    "    model.Add(s[2] >= 1)                #\n",
    "    model.Add(s[1] - s[2] > 0)          #   \n",
    "    \n",
    "    # SpliceAI limitation\n",
    "    # Absolutes\n",
    "    model.Add(s[4] < 0)                 #\n",
    "    model.Add(s[5] <= 0)                #\n",
    "    model.Add(s[6] >= 0)                #\n",
    "    # model.Add(s[7] >= 1)\n",
    "    model.Add(s[12] < 0)                #\n",
    "    model.Add(s[13] <= 0)               #\n",
    "    model.Add(s[14] >= 0)               #\n",
    "    # Order\n",
    "    model.Add(s[5] - s[4] > 0)          #\n",
    "    model.Add(s[6] - s[5] > 0)          #\n",
    "    model.Add(s[7] - s[6] > 0)          #\n",
    "    model.Add(s[13] - s[12] > 0)        #\n",
    "    model.Add(s[14] - s[13] > 0)        #\n",
    "    model.Add(s[4] >= s[12])            #\n",
    "\n",
    "    # under limit\n",
    "    # model.Add(s[1] + s[10] + s[12] >= 0)\n",
    "    # model.Add(s[4] <= s[10] + s[12])\n",
    "\n",
    "    # Absolutes\n",
    "    model.Add(s[11] >= 0)               #\n",
    "    # Score order\n",
    "    model.Add(s[9] >= s[7])             #\n",
    "    model.Add(s[8] - s[9] > 0)          #\n",
    "    model.Add(s[10] - s[8] > 0)         #\n",
    "    # model.Add(s[9] - s[11] > 0)         #\n",
    "    model.Add(s[9] - s[11] >= 0)         #\n",
    "\n",
    "\n",
    "    # ソルバの設定と解の探索\n",
    "    solver = cp_model.CpSolver()\n",
    "    solution_collector = SolutionCollector([s[i] for i in range(1, 15)])\n",
    "    solver.SearchForAllSolutions(model, solution_collector)\n",
    "    \n",
    "    # 全ての解を返す\n",
    "    return solution_collector.GetAllSolutions()\n",
    "\n",
    "# 解の計算\n",
    "all_solutions = find_all_solutions()\n",
    "print(f'Total solutions found: {len(all_solutions)}')\n",
    "\n",
    "# Debug and confirm the solutions\n",
    "# for index, solution in enumerate(all_solutions):\n",
    "#     print(f'Solution {index + 1}: {solution}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = 0.8\n",
    "random_state = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 18432, TN: 180958\n",
      "Filtered out [Warning] ENST_with_Ver_not_available, TP: 17258, TN: 167371\n"
     ]
    }
   ],
   "source": [
    "# prepare the results\n",
    "tp['is_Canonical'].replace({True: \"Yes\", False: \"No\"}, inplace=True)\n",
    "tn['is_Canonical'].replace({True: \"Yes\", False: \"No\"}, inplace=True)\n",
    "\n",
    "print(f\"TP: {len(tp)}, TN: {len(tn)}\")\n",
    "tp = tp[tp['ENST_Full'] != \"[Warning] ENST_with_Ver_not_available\"]\n",
    "tn = tn[tn['ENST_Full'] != \"[Warning] ENST_with_Ver_not_available\"]\n",
    "tp = tp[tp['maxsplai'] != \"NA\"]\n",
    "tn = tn[tn['maxsplai'] != \"NA\"]\n",
    "print(f\"Filtered out [Warning] ENST_with_Ver_not_available, TP: {len(tp)}, TN: {len(tn)}\")\n",
    "\n",
    "# SpliceType != [Warning] ENST_unmatch\n",
    "# tp = tp[tp['SpliceType'] != \"[Warning] ENST_unmatch\"]\n",
    "# tn = tn[tn['SpliceType'] != \"[Warning] ENST_unmatch\"]\n",
    "# print(f\"Filtered out [Warning] ENST_unmatch, TP: {len(tp)}, TN: {len(tn)}\")\n",
    "\n",
    "# Split the data into training and test sets\n",
    "tp_train = tp.sample(frac=frac, random_state=random_state)\n",
    "tp_test = tp.drop(tp_train.index)\n",
    "tn_train = tn.sample(frac=frac, random_state=random_state)\n",
    "tn_test = tn.drop(tn_train.index)\n",
    "\n",
    "# Save the dataframes as pickle files\n",
    "tp_train.to_pickle(f'train_test_pkls/tp_prescore_train_{random_state}.pkl')\n",
    "tp_test.to_pickle(f'train_test_pkls/tp_prescore_test_{random_state}.pkl')\n",
    "tn_train.to_pickle(f'train_test_pkls/tn_prescore_train_{random_state}.pkl')\n",
    "tn_test.to_pickle(f'train_test_pkls/tn_prescore_test_{random_state}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find optimal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###  Processed 0 solutions  ###\n",
      "###  Processed 50 solutions  ###\n",
      "###  Processed 100 solutions  ###\n",
      "###  Processed 150 solutions  ###\n",
      "###  Processed 200 solutions  ###\n",
      "###  Processed 250 solutions  ###\n",
      "###  Processed 300 solutions  ###\n",
      "###  Processed 350 solutions  ###\n",
      "###  Processed 400 solutions  ###\n",
      "###  Processed 450 solutions  ###\n",
      "###  Processed 500 solutions  ###\n",
      "###  Processed 550 solutions  ###\n",
      "###  Processed 600 solutions  ###\n",
      "###  Processed 650 solutions  ###\n",
      "###  Processed 700 solutions  ###\n",
      "###  Processed 750 solutions  ###\n",
      "###  Processed 800 solutions  ###\n",
      "###  Processed 850 solutions  ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x1080cf700>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Volumes/SSD_480GB/utsu/miniconda3/envs/nar/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "buf: float = 0.978\n",
    "\n",
    "for i, solution in enumerate(all_solutions):\n",
    "    ths_scores = {'clinvar_same_pos': solution['s1'],\n",
    "            'clinvar_same_motif': solution['s2'],\n",
    "            'clinvar_else': solution['s3'],\n",
    "            'non_canon_splai_lte_0.1_outside': solution['s4'],    \n",
    "            'non_canon_splai_lte_0.1_other': solution['s5'],\n",
    "            'non_canon_splai_bet_0.1_0.2': solution['s6'],\n",
    "            'non_canon_splai_gte_0.2': solution['s7'],\n",
    "            'canon_strong': solution['s8'], \n",
    "            'canon_moderate': solution['s9'], \n",
    "            'frameshift_nmd_eloF': solution['s10'], \n",
    "            'frameshift_nmd_not_eloF': solution['s11'],\n",
    "            'canon_splai_lte_0.1': solution['s12'],\n",
    "            'canon_splai_bet_0.1_0.2': solution['s13'],\n",
    "            'canon_splai_gte_0.2': solution['s14']\n",
    "            }\n",
    "    \n",
    "    scoring = Scoring(ths=ths_scores)\n",
    "\n",
    "    tp_train = pd.read_pickle(f'train_test_pkls/tp_prescore_train_{random_state}.pkl')\n",
    "    tn_train = pd.read_pickle(f'train_test_pkls/tn_prescore_train_{random_state}.pkl')\n",
    "\n",
    "    tp_train['insilico_screening'] = tp_train.parallel_apply(scoring.insilico_screening, axis=1)\n",
    "    tp_train['clinvar_screening'] = tp_train.parallel_apply(scoring.clinvar_screening, axis=1)\n",
    "    tp_train['PriorityScore'] = tp_train.parallel_apply(scoring.calc_priority_score, axis=1)\n",
    "    # tp_train = scoring.calc_priority_score(tp_train)\n",
    "\n",
    "    tn_train['insilico_screening'] = tn_train.parallel_apply(scoring.insilico_screening, axis=1)\n",
    "    tn_train['clinvar_screening'] = tn_train.parallel_apply(scoring.clinvar_screening, axis=1)\n",
    "    tn_train['PriorityScore'] = tn_train.parallel_apply(scoring.calc_priority_score, axis=1)\n",
    "    # tn_train = scoring.calc_priority_score(tn_train)\n",
    "\n",
    "    # Extract the columns needed\n",
    "    tp_train = tp_train[['variant_id', 'LABEL', 'PriorityScore', 'maxsplai']]\n",
    "    tn_train = tn_train[['variant_id', 'LABEL', 'PriorityScore', 'maxsplai']]\n",
    "\n",
    "    ### ========================================================== ##\n",
    "    data = pd.concat([tp_train, tn_train], ignore_index=True)\n",
    "    data.drop_duplicates(subset='variant_id', keep=False, inplace=True)\n",
    "\n",
    "    # Cast the columns to float type\n",
    "    data['LABEL'] = data['LABEL'].astype(int)\n",
    "    # Extract rows with PriorityScore not 'Not available'\n",
    "    data = data[data['PriorityScore'] != 'Not available']\n",
    "    data['PriorityScore'] = data['PriorityScore'].astype(float)\n",
    "    data['maxsplai'] = data['maxsplai'].astype(float)\n",
    "\n",
    "    ## DeLong test and AUC confidence interval\n",
    "    ground_truth = np.array(data['LABEL'])\n",
    "    predictions_fw = np.array(data['PriorityScore'])\n",
    "\n",
    "    auc1, var1 = delong_roc_variance(ground_truth, predictions_fw)\n",
    "    cilower1, ciupper1 = compute_auc_confidence_interval(auc1, var1)\n",
    "\n",
    "    results.append(\n",
    "        {'index': i+1, 's1': solution['s1'], 's2': solution['s2'], \n",
    "         's3': solution['s3'], 's4': solution['s4'], 's5': solution['s5'], \n",
    "         's6': solution['s6'], 's7': solution['s7'], 's8': solution['s8'], \n",
    "         's9': solution['s9'], 's10': solution['s10'], 's11': solution['s11'], \n",
    "         's12': solution['s12'], 's13': solution['s13'], 's14': solution['s14'],\n",
    "         'auROC': f\"{auc1:.10f}, '95% Confidence Interval': {cilower1:.12f}-{ciupper1:.12f}\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if auc1 > buf:\n",
    "        buf = auc1\n",
    "        print(f\"\\n===== New best AUC: {auc1:.10f} with solution {i+1} =======\")\n",
    "        print(f\"New best solution {i}: {solution} \\n\")\n",
    "        predictions_sp = np.array(data['maxsplai'])\n",
    "        auc2, var2 = delong_roc_variance(ground_truth, predictions_sp)\n",
    "        cilower2, ciupper2 = compute_auc_confidence_interval(auc2, var2)\n",
    "        p_value_log = delong_roc_test(ground_truth, predictions_fw, predictions_sp)\n",
    "        print(f\"AUC - Framework (95%CI): {auc1:.3f} [{cilower1:.4f}-{ciupper1:.4f}]\")\n",
    "        print(f\"AUC - SpliceAI (95%CI) : {auc2:.3f} [{cilower2:.4f}-{ciupper2:.4f}]\")\n",
    "        print(f\"p-value (DeLong Test)  : {10**p_value_log[0][0]:.2e}\\n\")\n",
    "        print(\"===========================================================\")\n",
    "        \n",
    "    if i % 50 == 0:\n",
    "        print(f\"###  Processed {i} solutions  ###\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance test using test variant set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "thsdict = {\n",
    "    'opti': \n",
    "            {'clinvar_same_pos': 2,\n",
    "             'clinvar_same_motif': 1,\n",
    "             'clinvar_else': 0,\n",
    "             'non_canon_splai_lte_0.1_outside': -3,\n",
    "             'non_canon_splai_lte_0.1_other': -2,\n",
    "             'non_canon_splai_bet_0.1_0.2': 1,\n",
    "             'non_canon_splai_gte_0.2': 2,\n",
    "             'canon_strong': 6, \n",
    "             'canon_moderate': 5, \n",
    "             'frameshift_nmd_eloF': 7, \n",
    "             'frameshift_nmd_not_eloF': 3,\n",
    "             'canon_splai_lte_0.1': -3,\n",
    "             'canon_splai_bet_0.1_0.2': -1,\n",
    "             'canon_splai_gte_0.2': 0},\n",
    "}\n",
    "\n",
    "thsdict = {\n",
    "    'opti': \n",
    "            {'clinvar_same_pos': 2,\n",
    "             'clinvar_same_motif': 1,\n",
    "             'clinvar_else': 0,\n",
    "             'non_canon_splai_lte_0.1_outside': -1,\n",
    "             'non_canon_splai_lte_0.1_other': 0,\n",
    "             'non_canon_splai_bet_0.1_0.2': 1,\n",
    "             'non_canon_splai_gte_0.2': 3,\n",
    "             'canon_strong': 5, \n",
    "             'canon_moderate': 4, \n",
    "             'frameshift_nmd_eloF': 6, \n",
    "             'frameshift_nmd_not_eloF': 2,\n",
    "             'canon_splai_lte_0.1': -2,\n",
    "             'canon_splai_bet_0.1_0.2': 0,\n",
    "             'canon_splai_gte_0.2': 1},\n",
    "}\n",
    "\n",
    "\n",
    "# Memo\n",
    "# {'s1': 2, 's2': 1, 's3': 0, 's4': -1, 's5': 0, 's6': 1, 's7': 3, 's8': 5, 's9': 4, 's10': 6, 's11': 2, 's12': -2, 's13': 0, 's14': 1}\n",
    "\n",
    "# thsdict = {\n",
    "#     'opti': \n",
    "#             {'clinvar_same_pos': 3,\n",
    "#              'clinvar_same_motif': 2,\n",
    "#              'clinvar_else': 0,\n",
    "#              'non_canon_splai_lte_0.1_outside': -4,\n",
    "#              'non_canon_splai_lte_0.1_other': -3,\n",
    "#              'non_canon_splai_bet_0.1_0.2': 3,\n",
    "#              'non_canon_splai_gte_0.2': 4,\n",
    "#              'canon_strong': 5, \n",
    "#              'canon_moderate': 4, \n",
    "#              'frameshift_nmd_eloF': 6, \n",
    "#              'frameshift_nmd_not_eloF': 4,\n",
    "#              'canon_splai_lte_0.1': -4,\n",
    "#              'canon_splai_bet_0.1_0.2': -1,\n",
    "#              'canon_splai_gte_0.2': 0},\n",
    "# }\n",
    "\n",
    "tp_test = pd.read_pickle(f'train_test_pkls/tp_prescore_test_{random_state}.pkl')\n",
    "tn_test = pd.read_pickle(f'train_test_pkls/tn_prescore_test_{random_state}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC - Framework (95%CI): 0.981 [0.9781-0.9831]\n",
      "AUC - SpliceAI (95%CI) : 0.979 [0.9759-0.9821]\n",
      "p-value (DeLong Test)  : 1.99e-01\n"
     ]
    }
   ],
   "source": [
    "ths = thsdict['opti']\n",
    "scoring = Scoring(ths=ths)\n",
    "\n",
    "tp_test['insilico_screening'] = tp_test.parallel_apply(scoring.insilico_screening, axis=1)\n",
    "tp_test['clinvar_screening'] = tp_test.parallel_apply(scoring.clinvar_screening, axis=1)\n",
    "tp_test['PriorityScore'] = tp_test.parallel_apply(scoring.calc_priority_score, axis=1)\n",
    "# tp_test = scoring.calc_priority_score2(tp_test)\n",
    "\n",
    "\n",
    "tn_test['insilico_screening'] = tn_test.parallel_apply(scoring.insilico_screening, axis=1)\n",
    "tn_test['clinvar_screening'] = tn_test.parallel_apply(scoring.clinvar_screening, axis=1)\n",
    "tn_test['PriorityScore'] = tn_test.parallel_apply(scoring.calc_priority_score, axis=1)\n",
    "# tn_test = scoring.calc_priority_score2(tn_test)\n",
    "\n",
    "\n",
    "# Extract the columns needed\n",
    "tp_test = tp_test[['variant_id', 'LABEL', 'PriorityScore', 'maxsplai']]\n",
    "tn_test = tn_test[['variant_id', 'LABEL', 'PriorityScore', 'maxsplai']]\n",
    "\n",
    "### ========================================================== ##\n",
    "data = pd.concat([tp_test, tn_test], ignore_index=True)\n",
    "data.drop_duplicates(subset='variant_id', keep=False, inplace=True)\n",
    "\n",
    "# Cast the columns to float type\n",
    "data['LABEL'] = data['LABEL'].astype(int)\n",
    "data = data[data['PriorityScore'] != 'Not available']   # Exclude rows with PriorityScore not 'Not available'\n",
    "data['PriorityScore'] = data['PriorityScore'].astype(float)\n",
    "data['maxsplai'] = data['maxsplai'].astype(float)\n",
    "\n",
    "# Plot the sensitivity and specificity for each threshold\n",
    "results_df = specificity_sensitivity_plotly(data)\n",
    "fig_opti = plot_sensitivity_specificity_plotly(results_df, 800, 800)\n",
    "# fig2 = plot_sensitivity_specificity_plotly_without_legened(results_df)\n",
    "# print(tp_test['PriorityScore'].isnull().sum(), tn_test['PriorityScore'].isnull().sum())\n",
    "\n",
    "## DeLong test and AUC confidence interval\n",
    "ground_truth = np.array(data['LABEL'])\n",
    "predictions_fw = np.array(data['PriorityScore'])\n",
    "predictions_sp = np.array(data['maxsplai'])\n",
    "\n",
    "auc1, var1 = delong_roc_variance(ground_truth, predictions_fw)\n",
    "cilower1, ciupper1 = compute_auc_confidence_interval(auc1, var1)\n",
    "auc2, var2 = delong_roc_variance(ground_truth, predictions_sp)\n",
    "cilower2, ciupper2 = compute_auc_confidence_interval(auc2, var2)\n",
    "\n",
    "p_value_log = delong_roc_test(ground_truth, predictions_fw, predictions_sp)\n",
    "\n",
    "print(f\"AUC - Framework (95%CI): {auc1:.3f} [{cilower1:.4f}-{ciupper1:.4f}]\")\n",
    "print(f\"AUC - SpliceAI (95%CI) : {auc2:.3f} [{cilower2:.4f}-{ciupper2:.4f}]\")\n",
    "print(f\"p-value (DeLong Test)  : {10**p_value_log[0][0]:.2e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
