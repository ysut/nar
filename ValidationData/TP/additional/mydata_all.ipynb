{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda create -n sss python=3.8 -y && conda activate sss\n",
    "# conda install -y -c bioconda gffutils jupyter tqdm cyvcf2 pathlib2 pandarallel pysam liftover pybedtools\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from Bio.Seq import Seq\n",
    "# from liftover import get_lifter\n",
    "from pathlib2 import Path\n",
    "from pandarallel import pandarallel\n",
    "from tqdm import tqdm\n",
    "import gffutils\n",
    "import pysam\n",
    "from cyvcf2 import VCF\n",
    "\n",
    "### Logging setup\n",
    "from logging import getLogger, config\n",
    "import yaml\n",
    "parent_directory = os.path.dirname(os.path.dirname('__file__'))\n",
    "config_path: str = os.path.join(parent_directory, '../../../config/logging.yaml')\n",
    "with open(config_path, 'r') as f:\n",
    "    config.dictConfig(yaml.safe_load(f))\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "########   Initialize and setup pandas methods   ########\n",
    "os.environ['JOBLIB_TEMP_FOLDER'] = '/tmp' \n",
    "pandarallel.initialize(nb_workers=5, progress_bar=True, verbose=1, use_memory_fs=False) \n",
    "tqdm.pandas()\n",
    "\n",
    "import sys\n",
    "try: \n",
    "    __file__\n",
    "    sys.path.append(os.path.join(os.path.dirname('__file__')))\n",
    "except NameError:\n",
    "    Path().resolve()\n",
    "    sys.path.append(os.path.join(Path().resolve(), '../../../'))\n",
    "\n",
    "from libs import utils, preprocess, variantfilter, posparser, splaiparser\n",
    "# from libs import predeffect, scoring\n",
    "from libs import anno_spliceai, anno_clinvar\n",
    "from libs.deco import print_filtering_count\n",
    "# from libs import predeffect\n",
    "from libs.scoring import Scoring\n",
    "from libs import predeffect\n",
    "\n",
    "\n",
    "gencode_gff = '../../../Resources/05_GENCODE_v43lift37/gencode.v43lift37.annotation.sort.gff3.gz'\n",
    "\n",
    "try:\n",
    "    db_anno_gencode = '../../../Resources/06_gffutilsdb/gencode.v43lift37.annotation.gtf.db'\n",
    "    db_anno_intron = '../../../Resources/06_gffutilsdb/gencode.v43lift37.annotation.intron.gtf.db'\n",
    "    db = gffutils.FeatureDB(db_anno_gencode)\n",
    "    db_intron = gffutils.FeatureDB(db_anno_intron)\n",
    "except ValueError:\n",
    "    db_anno_gencode = '/resources/DBs/gencode.v43lift37.annotation.gtf.db'\n",
    "    db_anno_intron = '/resources/DBs/gencode.v43lift37.annotation.intron.gtf.db'\n",
    "    db = gffutils.FeatureDB(db_anno_gencode)\n",
    "    db_intron = gffutils.FeatureDB(db_anno_intron)\n",
    "\n",
    "## Thresholds configuration\n",
    "thresholds_SpliceAI_parser: dict = {\n",
    "    'TH_min_sALDL': 0.02, 'TH_max_sALDL': 0.2, \n",
    "    'TH_min_sAGDG': 0.01, 'TH_max_sAGDG': 0.05,\n",
    "    'TH_min_GExon': 25, 'TH_max_GExon': 500,\n",
    "    'TH_sAG': 0.2, 'TH_sDG': 0.2\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse VCF and annotate ENST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_vcf: str = \"/Users/utsu/work/Github/nar/mydata/mydata.splai.pangolin.vep.maxentscan.loftee.vcf\"\n",
    "fp = Path(raw_vcf)\n",
    "fp_stem, fp_dir = fp.stem, fp.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Parse VCF to simple input table\n",
    "vcf = VCF(raw_vcf)\n",
    "header = vcf.header_iter()\n",
    "for h in header:\n",
    "    try:\n",
    "        h['ID']\n",
    "    except KeyError:\n",
    "        continue\n",
    "    else:\n",
    "        if h['ID'] == 'CSQ':\n",
    "            vep_cols_list = h['Description'].split('Format: ')[1].rstrip('\"').split('|')\n",
    "        elif h['ID'] == 'SpliceAI':\n",
    "            splai_cols_list = h['Description'].split('Format: ')[1].rstrip('\"').split('|')\n",
    "        elif h['ID'] == 'Pangolin':\n",
    "            pang_cols_list = h['Description'].split('Format: ')[1].rstrip('\"').split('|')\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "vepidx: dict = {col: i for i, col in enumerate(vep_cols_list)}\n",
    "splaidx: dict = {col: i for i, col in enumerate(splai_cols_list)}\n",
    "\n",
    "cols = [\n",
    "    'CHROM', 'POS', 'REF', 'ALT', 'GeneSymbol', 'SymbolSource', 'HGNC_ID', \n",
    "    'ENST', 'HGVSc', 'Consequence', 'EXON', 'INTRON', 'Strand',\n",
    "    'DS_AG', 'DS_AL', 'DS_DG', 'DS_DL', \n",
    "    'DP_AG', 'DP_AL', 'DP_DG', 'DP_DL', 'maxsplai',\n",
    "    'loftee', 'maxentscan_alt', 'maxentscan_diff', 'maxentscan_ref', \n",
    "    'pang_gene', 'pang_pos_socre_gain', 'pang_pos_score_loss', 'pang_warning', \n",
    "    'maxpangolin'\n",
    "]\n",
    "\n",
    "df: pd.DataFrame = pd.DataFrame(columns=cols)\n",
    "for v in VCF(raw_vcf):\n",
    "    vep: list = v.INFO.get('CSQ').split('|')\n",
    "\n",
    "    # Get HGVSc from VEP\n",
    "    try:\n",
    "        hgvsc = re.search('(?<=:).*',vep[vepidx['HGVSc']])[0]\n",
    "    except TypeError:\n",
    "        hgvsc = \"NA\"\n",
    "\n",
    "    # Get SpliceAI scores\n",
    "    if v.INFO.get('SpliceAI'):\n",
    "        splai: list = v.INFO.get('SpliceAI').split(',')[0].split('|')\n",
    "    else:\n",
    "        splai = ['NA'] * len(splai_cols_list)\n",
    "\n",
    "    # Get Pangolin scores\n",
    "    if v.INFO.get('Pangolin'):\n",
    "        pangolin: list = v.INFO.get('Pangolin').split('|')\n",
    "    else:\n",
    "        pangolin = ['NA'] * len(pang_cols_list)\n",
    "        \n",
    "    # Get Squirls scores\n",
    "    if v.INFO.get('SQUIRLS_SCORE'):\n",
    "        squirls: float = v.INFO.get('SQUIRLS_SCORE')\n",
    "    else:\n",
    "        squirls = \"NA\"\n",
    "\n",
    "    # Convert strand to +/- \n",
    "    strand = lambda s: '+' if s == '1' else '-'\n",
    "\n",
    "    # Get max SpliceAI scores\n",
    "    ds_ag: float = splai[splaidx['DS_AG']]\n",
    "    ds_al: float = splai[splaidx['DS_AL']]\n",
    "    ds_dg: float = splai[splaidx['DS_DG']]\n",
    "    ds_dl: float = splai[splaidx['DS_DL']]\n",
    "    if splai[splaidx['DP_AG']] == 'NA':\n",
    "        maxsplai: str = \"NA\"\n",
    "    maxsplai: float = max(ds_ag, ds_al, ds_dg, ds_dl)\n",
    "\n",
    "    # Get Pangplin scores\n",
    "    pang_gene: str = pangolin[0]\n",
    "    pang_pos_score_gain: str = pangolin[1]\n",
    "    pang_pos_score_loss: str = pangolin[2]\n",
    "    pang_warning: str = pangolin[3]\n",
    "    if pang_gene == 'NA':\n",
    "        maxpangolin = \"NA\"\n",
    "    else:\n",
    "        maxpangolin = max(\n",
    "            np.abs(float(pang_pos_score_gain.split(':')[1])), \n",
    "            np.abs(float(pang_pos_score_loss.split(':')[1]))\n",
    "            )\n",
    "\n",
    "    # Add df row\n",
    "    df = pd.concat([df, pd.DataFrame([[\n",
    "        v.CHROM, v.POS, v.REF, v.ALT[0], \n",
    "        vep[vepidx['SYMBOL']], vep[vepidx['SYMBOL_SOURCE']], \n",
    "        vep[vepidx['HGNC_ID']], vep[vepidx['Feature']], hgvsc, \n",
    "        vep[vepidx['Consequence']], \n",
    "        vep[vepidx['EXON']], vep[vepidx['INTRON']],\n",
    "        strand(vep[vepidx['STRAND']]), \n",
    "        ds_ag, ds_al, ds_dg, ds_dl,\n",
    "        splai[splaidx['DP_AG']], splai[splaidx['DP_AL']], \n",
    "        splai[splaidx['DP_DG']], splai[splaidx['DP_DL']],\n",
    "        maxsplai, vep[vepidx['LoF']], \n",
    "        vep[vepidx['MaxEntScan_alt']], \n",
    "        vep[vepidx['MaxEntScan_diff']], \n",
    "        vep[vepidx['MaxEntScan_ref']],\n",
    "        pang_gene, pang_pos_score_gain, pang_pos_score_loss, pang_warning,\n",
    "        maxpangolin]],\n",
    "        columns=cols)], ignore_index=True)\n",
    "        \n",
    "\n",
    "df['ENST_Full'] = df.apply(posparser.fetch_enst_full, db=db, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(f\"{fp_dir}/{fp_stem}.enst.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ここから解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/01/09 04:22:37 [INFO   ] (__main__) - Calculate the distance to the nearest splice site in intron variant...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16353/16353 [00:08<00:00, 1851.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/01/09 04:22:46 [INFO   ] (__main__) - Classify \"Canonical\" splice site or \"Non-canonical\" splice site...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 16353/16353 [00:06<00:00, 2706.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/01/09 04:22:53 [INFO   ] (__main__) - Annotating splicing information...\n",
      "2025/01/09 04:22:53 [INFO   ] (__main__) - Annotating ClinVar varaints interpretations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16353/16353 [00:00<00:00, 19068.08it/s]\n",
      "100%|██████████| 16353/16353 [00:00<00:00, 17876.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/01/09 04:22:55 [INFO   ] (__main__) - Parsing SpliceAI results...\n",
      "2025/01/09 04:22:55 [INFO   ] (__main__) - Annotating Exon/Intron position information...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16353/16353 [00:10<00:00, 1496.26it/s]\n",
      "100%|██████████| 16353/16353 [00:00<00:00, 18399.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/01/09 04:23:09 [INFO   ] (__main__) - Annotating aberrant splicing size (bp)...\n",
      "2025/01/09 04:23:10 [INFO   ] (__main__) - Predicting CDS change...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16353/16353 [00:07<00:00, 2290.53it/s]\n",
      "100%|██████████| 16353/16353 [00:00<00:00, 232859.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 1-6453453-G-A, Cannot predict splicing event, nan\n",
      "Error: 1-6453455-G-C, Cannot predict splicing event, nan\n",
      "Error: 1-33476442-A-G, Cannot predict splicing event, nan\n",
      "Error: 2-214143205-G-A, Cannot predict splicing event, nan\n",
      "Error: 2-214143209-T-C, Cannot predict splicing event, nan\n",
      "Error: 4-8951467-G-T, Cannot predict splicing event, nan\n",
      "Error: 5-121515251-A-G, Cannot predict splicing event, nan\n",
      "Error: 6-168377291-C-T, Cannot predict splicing event, nan\n",
      "Error: 6-168377291-C-T, Cannot predict splicing event, nan\n",
      "Error: 7-99205437-A-G, Cannot predict splicing event, nan\n",
      "Error: 8-7154903-T-C, Cannot predict splicing event, nan\n",
      "Error: 8-7155201-G-A, Cannot predict splicing event, nan\n",
      "Error: 8-7155242-C-A, Cannot predict splicing event, nan\n",
      "Error: 8-7155253-C-A, Cannot predict splicing event, nan\n",
      "Error: 8-7155522-G-C, Cannot predict splicing event, nan\n",
      "Error: 8-43147777-C-T, Cannot predict splicing event, nan\n",
      "Error: 8-43197018-T-C, Cannot predict splicing event, nan\n",
      "Error: 8-43197471-G-A, Cannot predict splicing event, nan\n",
      "Error: 9-6330978-T-C, Cannot predict splicing event, nan\n",
      "Error: 10-88769022-G-A, Cannot predict splicing event, nan\n",
      "Error: 10-88769341-A-G, Cannot predict splicing event, nan\n",
      "Error: 10-88769474-T-C, Cannot predict splicing event, nan\n",
      "Error: 10-88769498-C-T, Cannot predict splicing event, nan\n",
      "Error: 10-135490978-T-C, Cannot predict splicing event, nan\n",
      "Error: 10-135491117-A-G, Cannot predict splicing event, nan\n",
      "Error: 10-135491251-G-T, Cannot predict splicing event, nan\n",
      "Error: 10-135491282-G-A, Cannot predict splicing event, nan\n",
      "Error: 11-48373726-A-G, Cannot predict splicing event, nan\n",
      "Error: 11-48373822-A-C, Cannot predict splicing event, nan\n",
      "Error: 11-94245748-T-A, Cannot predict splicing event, nan\n",
      "Error: 12-25150183-T-C, Cannot predict splicing event, nan\n",
      "Error: 14-24507056-G-C, Cannot predict splicing event, nan\n",
      "Error: 15-31515788-G-A, Cannot predict splicing event, nan\n",
      "Error: 16-830845-T-G, Cannot predict splicing event, nan\n",
      "Error: 16-2849033-G-C, Cannot predict splicing event, nan\n",
      "Error: 17-17326396-G-A, Cannot predict splicing event, nan\n",
      "Error: 18-55309186-C-T, Cannot predict splicing event, nan\n",
      "Error: 18-55309285-G-C, Cannot predict splicing event, nan\n",
      "Error: 20-31238917-G-A, Cannot predict splicing event, nan\n",
      "Error: 20-42984454-G-A, Cannot predict splicing event, nan\n",
      "Error: 21-11097593-C-T, Cannot predict splicing event, nan\n",
      "Error: X-24330117-T-A, Cannot predict splicing event, nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/01/09 04:23:18 [INFO   ] (__main__) - Annotating CCRs info...\n",
      "2025/01/09 04:23:18 [INFO   ] (__main__) - Annotate CCR score\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(f\"{fp_dir}/{fp_stem}.enst.pkl\")\n",
    "\n",
    "df = df.fillna(\n",
    "    {'loftee': '.', 'maxentscan_alt': '.', 'maxentscan_diff': '.', \n",
    "        'maxentscan_ref': '.', 'pang_gene': '.', 'pang_pos_socre_gain': '.',\n",
    "        'pang_pos_score_loss': '.', 'pang_warning': '.'}\n",
    "        )\n",
    "\n",
    "logger.info('Calculate the distance to the nearest splice site in intron variant...')\n",
    "df['IntronDist'] = df.progress_apply(\n",
    "    posparser.signed_distance_to_exon_boundary, \n",
    "    db=db, db_intron=db_intron, axis=1)\n",
    "\n",
    "logger.info('Classify \"Canonical\" splice site or \"Non-canonical\" splice site...')\n",
    "df = posparser.classifying_canonical(df)\n",
    "\n",
    "df['Ex_or_Int'] = np.where(\n",
    "    df['IntronDist'] == \"[Warning] Invalid ENST ID\", \"[Warning] Invalid ENST ID\",\n",
    "    np.where(df['IntronDist'].isnull(), 'Exonic', 'Intronic'))\n",
    "\n",
    "tbx_anno = pysam.TabixFile(gencode_gff)\n",
    "df['exon_loc'] = df.progress_apply(\n",
    "    posparser.calc_exon_loc, tabixfile=tbx_anno, enstcolname='ENST', axis=1)\n",
    "df = pd.concat([df, df['exon_loc'].str.split(':', expand=True)], axis=1)\n",
    "df.rename(columns={0: 'ex_up_dist', 1: 'ex_down_dist'}, inplace=True)\n",
    "df.drop(columns=['exon_loc'], inplace=True)\n",
    "\n",
    "#2-2. Select minimum distance from upstream distance and downstream distance\n",
    "df['exon_pos'] = df.parallel_apply(posparser.select_exon_pos, axis=1)\n",
    "#2-3. Relative exon location\n",
    "df['prc_exon_loc'] = df.parallel_apply(posparser.calc_prc_exon_loc, axis=1)\n",
    "\n",
    "#2-4. Decision exonic splice sites (1 nt in acceptor site or 3 nts on Donor site)\n",
    "df['exon_splice_site'] = df.parallel_apply(posparser.extract_splicing_region, axis=1)\n",
    "\n",
    "#3.   Additional Splicing information\n",
    "logger.info('Annotating splicing information...')\n",
    "#3-1. Annotate splicing type ('Exonic Acceptor' etc.)\n",
    "df['SpliceType'] = df.parallel_apply(posparser.select_donor_acceptor, axis=1)\n",
    "\n",
    "#5.   Annotate ClinVar varaints interpretations\n",
    "logger.info('Annotating ClinVar varaints interpretations...')\n",
    "clinvar_file = '../../../clinvar/Filtered_BCF_GRCh37_20241211-044124/clinvar_GRCh37.germline.nocoflicted.bcf.gz'\n",
    "cln_bcf = pysam.VariantFile(clinvar_file)\n",
    "df['clinvar_same_pos'] = df.progress_apply(\n",
    "    anno_clinvar.anno_same_pos_vars, cln_bcf=cln_bcf, axis=1)\n",
    "df['clinvar_same_motif'] = df.progress_apply(\n",
    "    anno_clinvar.anno_same_motif_vars, cln_bcf=cln_bcf, axis=1)\n",
    "df['same_motif_clinsigs'] = df['clinvar_same_motif'].parallel_apply(\n",
    "    anno_clinvar.extract_same_motif_clinsigs)\n",
    "\n",
    "logger.info('Parsing SpliceAI results...')\n",
    "logger.info('Annotating Exon/Intron position information...')\n",
    "df['ExInt_INFO'] = df.progress_apply(\n",
    "    splaiparser.calc_exint_info, db=db, db_intron=db_intron, axis=1)\n",
    "\n",
    "#6-3. Predict splicing effects\n",
    "df['Pseudoexon'] = df.progress_apply(\n",
    "    splaiparser.pseudoexon_activation,\n",
    "    thresholds=thresholds_SpliceAI_parser, \n",
    "    db_intron=db_intron,\n",
    "    axis=1)\n",
    "\n",
    "df['Part_IntRet'] = df.parallel_apply(\n",
    "    splaiparser.partial_intron_retention,\n",
    "    thresholds=thresholds_SpliceAI_parser, \n",
    "    axis=1)\n",
    "\n",
    "df['Part_ExDel'] = df.parallel_apply(\n",
    "    splaiparser.partial_exon_deletion,\n",
    "    thresholds=thresholds_SpliceAI_parser, \n",
    "    axis=1)\n",
    "\n",
    "df['Exon_skipping'] = df.parallel_apply(\n",
    "    splaiparser.exon_skipping, \n",
    "    thresholds=thresholds_SpliceAI_parser, \n",
    "    axis=1)\n",
    "                                        \n",
    "df['Int_Retention'] = df.parallel_apply(\n",
    "    splaiparser.intron_retention, \n",
    "    thresholds=thresholds_SpliceAI_parser, \n",
    "    axis=1)\n",
    "\n",
    "df['multiexs'] = df.parallel_apply(\n",
    "    splaiparser.multi_exon_skipping, \n",
    "    thresholds=thresholds_SpliceAI_parser, \n",
    "    axis=1)\n",
    "\n",
    "#7.   Annotate aberrant splicing size (bp)\n",
    "logger.info('Annotating aberrant splicing size (bp)...')\n",
    "#7-1. Annotate size of \n",
    "df['Size_Part_ExDel'] = df.parallel_apply(\n",
    "    splaiparser.anno_partial_exon_del_size, \n",
    "    thresholds=thresholds_SpliceAI_parser, \n",
    "    axis=1)\n",
    "\n",
    "#7-3. Annotate size of partial intron retention\n",
    "df['Size_Part_IntRet'] = df.parallel_apply(\n",
    "    splaiparser.anno_partial_intron_retention_size, \n",
    "    thresholds=thresholds_SpliceAI_parser,\n",
    "    axis=1)\n",
    "\n",
    "#7-2. Annotate size of pseudoexon\n",
    "df['Size_pseudoexon'] = df.parallel_apply(\n",
    "    splaiparser.anno_gained_exon_size, \n",
    "    thresholds=thresholds_SpliceAI_parser, \n",
    "    axis=1)\n",
    "\n",
    "#7-4. Annotate size of intron retention\n",
    "df['Size_IntRet'] = df.parallel_apply(\n",
    "    splaiparser.anno_intron_retention_size, \n",
    "    thresholds=thresholds_SpliceAI_parser,\n",
    "    axis=1)\n",
    "\n",
    "#7-5. Annotate size of exon skipping\n",
    "df['Size_skipped_exon'] = df.parallel_apply(\n",
    "    splaiparser.anno_skipped_exon_size, \n",
    "    thresholds=thresholds_SpliceAI_parser,\n",
    "    axis=1)\n",
    "\n",
    "df['variant_id'] = df['CHROM'].astype(str) + '-' \\\n",
    "    + df['POS'].astype(str) + '-' + df['REF'] + '-' + df['ALT']\n",
    "\n",
    "#8.   Evaluate splicing effects\n",
    "logger.info('Predicting CDS change...')\n",
    "#8-1. Predict CDS change\n",
    "df['CDS_Length'] = df.progress_apply(predeffect.calc_cds_len, db=db, axis=1)\n",
    "df['is_10%_truncation'] = df.progress_apply(predeffect.calc_cds_len_shorten, axis=1)\n",
    "\n",
    "#8-2. Determine if the gene is included in eLoFs genes\n",
    "df['is_eLoF'] = df.parallel_apply(predeffect.elofs_judge, axis=1)\n",
    "\n",
    "#8-3. Determine causing NMD or not\n",
    "df['is_NMD_at_Canon'] = df.parallel_apply(predeffect.nmd_judge, axis=1)\n",
    "\n",
    "#8-4. Frame check\n",
    "# Covert to str (Cannot predict splicing event) to np.nan\n",
    "cannot_predict: str = 'Cannot predict splicing event'\n",
    "df['Size_Part_ExDel'] = df['Size_Part_ExDel'].replace(cannot_predict, np.nan)\n",
    "df['Size_Part_IntRet'] = df['Size_Part_IntRet'].replace(cannot_predict, np.nan)\n",
    "df['Size_pseudoexon'] = df['Size_pseudoexon'].replace(cannot_predict, np.nan)\n",
    "df['Size_IntRet'] = df['Size_IntRet'].replace(cannot_predict, np.nan)\n",
    "df['Size_skipped_exon'] = df['Size_skipped_exon'].replace(cannot_predict, np.nan)\n",
    "\n",
    "df['is_Frameshift_Part_ExDel'] = df['Size_Part_ExDel'].parallel_apply(\n",
    "    predeffect.frame_check)\n",
    "df['is_Frameshift_Part_IntRet'] = df['Size_Part_IntRet'].parallel_apply(\n",
    "    predeffect.frame_check)\n",
    "df['is_Frameshift_pseudoexon'] = df['Size_pseudoexon'].parallel_apply(\n",
    "    predeffect.frame_check)\n",
    "df['is_Frameshift_IntRet'] = df['Size_IntRet'].parallel_apply(\n",
    "    predeffect.frame_check)\n",
    "df['is_Frameshift_skipped_exon'] = df['Size_skipped_exon'].parallel_apply(\n",
    "    predeffect.frame_check)\n",
    "df['is_Frameshift'] = df[['is_Frameshift_Part_ExDel', \n",
    "                        'is_Frameshift_Part_IntRet', \n",
    "                        'is_Frameshift_pseudoexon', \n",
    "                        'is_Frameshift_IntRet', \n",
    "                        'is_Frameshift_skipped_exon'\n",
    "                        ]].any(axis=1)\n",
    "\n",
    "#9.   CCRs\n",
    "logger.info('Annotating CCRs info...')\n",
    "#9-1. Annotate truncated regions \n",
    "df['skipped_region'] = df.parallel_apply(\n",
    "    splaiparser.anno_skipped_regions, axis=1)\n",
    "df['deleted_region'] = df.parallel_apply(\n",
    "    splaiparser.anno_deleted_regions, \n",
    "    thresholds=thresholds_SpliceAI_parser, axis=1)\n",
    "\n",
    "#9-2. Intersect with CCRs\n",
    "logger.info('Annotate CCR score')\n",
    "df = predeffect.anno_ccr_score(df)\n",
    "\n",
    "# Extract data with SymbolSource == 'HGNC'\n",
    "# df = df[df['SymbolSource'] == 'HGNC']\n",
    "\n",
    "# df.to_pickle(f'splai_vep_vcfs/hgmd_dm/all_DM_chr{chr}.splai.vep.nondel.enst.prescore.hgnconly.pkl')\n",
    "# df.to_pickle(f\"variant_data_set_vcfs/hgmd_splai_maxent_pango_sq/hgmd_dm.chr{chr}.splai.vep.maxent.loftee.snv.pangolin.squirls.prescore.onlyhgnc.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Part_ExDel\n",
       "False                            16156\n",
       "True                               155\n",
       "Cannot predict splicing event       42\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Part_ExDel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(f\"{fp_dir}/{fp_stem}.enst.prescore.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(f\"{fp_dir}/{fp_stem}.enst.prescore.pkl\")\n",
    "\n",
    "scoring = Scoring()\n",
    "df['insilico_screening'] = df.parallel_apply(scoring.insilico_screening, axis=1)\n",
    "df['clinvar_screening'] = df.parallel_apply(scoring.clinvar_screening, axis=1)\n",
    "df['recalibrated_splai'] = df.parallel_apply(scoring.recal_scores_in_canon, axis=1)\n",
    "\n",
    "def map_and_calc_score(row, score_map: dict) -> int:\n",
    "    \"\"\"\n",
    "    Map the score to the solution\n",
    "    s1, s2, s3, and s15 are clinvar_screening\n",
    "    s4, s5, s6, s7, s8, s9, s10 and s11 are insilico_screening\n",
    "    s12, s13 and s14 are recalibrated_splai\n",
    "    PriortiyScore is the sum of the \"clinvar_screening\", \"insilico_screening\", and \"recalibrated_splai\"\n",
    "    \"\"\"\n",
    "    if row['insilico_screening'] == \"Not available\":\n",
    "        return np.nan\n",
    "\n",
    "    return int(score_map[row['recalibrated_splai']]) + int(score_map[row['insilico_screening']]) + int(score_map[row['clinvar_screening']])\n",
    "\n",
    "solution = {'s1': 7.0, 's2': 5.0, 's3': 0.0, 's4': -4.0, \n",
    "            's5': -3.0, 's6': 0.0, 's7': 2.0, 's8': 3.0, 's9': 2.0,\n",
    "            's10': 4.0, 's11': 2.0, 's12': -1.0, 's13': 0.0, 's14': 1.0, \n",
    "            's15': -5.0, 's0': 0.0}\n",
    "\n",
    "df['PriorityScore'] = df.parallel_apply(map_and_calc_score, args=(solution,), axis=1)\n",
    "df.to_pickle(f\"{fp_dir}/{fp_stem}.enst.scored.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract screen positive variants only (PriorityScore >= 1)\n",
    "df = pd.read_pickle(f\"{fp_dir}/{fp_stem}.enst.scored.pkl\")\n",
    "df = df[df['PriorityScore'] >= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De novo filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of variants: 16353\n",
      "Start extract_denovo\n",
      "Filtering : 16353 --> 9944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def anno_hgmd(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    path_to_hgmd = '../../../Resources/07_HGMD_GeneBasedInfo/HGMD_GeneBasedInfo_2024.1.tsv.gz'\n",
    "    hgmd = pd.read_table(path_to_hgmd, header=0, dtype=str)\n",
    "    hgmd = hgmd[['gene', 'altsymbol', 'refseq', \n",
    "                 'expected_inheritance', 'hgncID', 'omimid', 'DM']]\n",
    "    hgmd = hgmd.astype({'DM': 'float64'})\n",
    "    df = pd.merge(df, hgmd, left_on='HGNC_ID', right_on='hgncID', how='left')\n",
    "    return df\n",
    "\n",
    "def anno_sf(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_sf = pd.read_table('../../../Resources/ACMG_SFv3.2.txt', header=0, dtype=str)\n",
    "    df_sf = df_sf[['Gene', 'Disease/Phentyope', 'Inheritance ', 'Variants to report']]\n",
    "    df = pd.merge(df, df_sf, left_on='GeneSymbol', right_on='Gene', how='left')\n",
    "    return df\n",
    "\n",
    "mydata = './original.snpeff.state.disease.identifiedgene.filtered.splai.tsv'\n",
    "df = pd.read_table(mydata, sep='\\t', dtype=str)\n",
    "print(f\"Total number of variants: {len(df)}\")\n",
    "df = variantfilter.extract_denovo(df)\n",
    "df.loc[:,'is_denovo'] = True\n",
    "df = df[df['vqslod'] > -7.18]\n",
    "df = df[((df['denovogear'] > 0.02) | (df['denovogear'].isnull()))\n",
    "        & ((df['triodenovo'] > 5.72) | (df['triodenovo'].isnull()))\n",
    "        & ((df['dnmfilter'] > 0.196) | (df['dnmfilter'].isnull()))]\n",
    "df.drop(columns=['variant_id', 'ID_y'], inplace=True)\n",
    "df['variant_id'] = df['CHROM'] + '-' + df['POS'] + '-' + df['REF'] + '-' + df['ALT']\n",
    "\n",
    "# merge with mydata\n",
    "df_variant = pd.read_pickle(f\"{fp_dir}/{fp_stem}.enst.scored.pkl\")\n",
    "df_variant.drop_duplicates(subset=['variant_id'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variants after filtering: 6406\n",
      "1138 + 2640\n",
      "1306\n",
      "Screen Positive: 237, Screen Negative: 2401\n"
     ]
    }
   ],
   "source": [
    "df = df[['sample', 'fa', 'mo', 'Disease', 'is_denovo', 'variant_id', 'State', 'Identified_Gene']]\n",
    "df = pd.merge(df, df_variant, on='variant_id')\n",
    "print(f\"Number of variants after filtering: {len(df)}\")\n",
    "\n",
    "df = anno_hgmd(df)\n",
    "df = anno_sf(df)\n",
    "df['skipped_ccrs'] = df['skipped_ccrs'].replace('.', np.nan).astype(float).copy()\n",
    "df['deleted_ccrs'] = df['deleted_ccrs'].replace('.', np.nan).astype(float)\n",
    "df['is_95%_CCRs'] = df.apply(lambda row: True if (row['skipped_ccrs'] > 95 or row['deleted_ccrs'] > 95) else False, axis=1)\n",
    "\n",
    "# Exclude error calling\n",
    "df = df.loc[df['variant_id'] != \"8-145138872-T-G\"]\n",
    "\n",
    "solved_case_ids = [\n",
    "    'Sample_4143', 'Sample_8803', 'Sample_17110', 'Sample_9768', 'Sample_16992',\n",
    "    'Sample_16970', 'Sample_4938', 'Sample_11555', 'Sample_10713']\n",
    "\n",
    "non_trios = [\n",
    "\t'Sample_17367', 'Sample_19880', 'Sample_7118', 'Sample_22831',\n",
    "\t'Sample_13784', 'Sample_8021', 'Sample_23636', 'Sample_5766', \n",
    "\t'Sample_12102', 'Sample_52', 'Sample_7700', 'Sample_3986', 'Sample_20591', \n",
    "\t'Sample_18910', 'Sample_11219', 'Sample_11895', 'Sample_10507', \n",
    "\t'Sample_14446', 'Sample_13089', 'Sample_2325', 'Sample_20287', \n",
    "\t'Sample_6024', 'Sample_16152', 'Sample_6584', 'Sample_10875', 'Sample_8436',\n",
    "\t'Sample_11750', 'Sample_13765', 'Sample_16783', 'Sample_15778']\n",
    "\n",
    "df = df.loc[~df['sample'].isin(non_trios)]\n",
    "df.loc[df['sample'].isin(solved_case_ids), 'State'] = 'Identified'\n",
    "df.loc[~df['Identified_Gene'].isnull(), 'State'] = 'Identified'\n",
    "\n",
    "df_solved = df[df['State'] == 'Identified']\n",
    "df_unsolved = df[df['State'] == 'Undetermined']\n",
    "\n",
    "print(f\"{len(df_solved)} + {len(df_unsolved)}\")\n",
    "print(len(df_unsolved['sample'].unique().tolist()))\n",
    "\n",
    "df = df.loc[df['State'] == 'Undetermined']\n",
    "df.rename(columns={'is_eLoF': 'eLoF', 'PriorityScore': 'Priority Score', \n",
    "                   'is_Canonical': 'Canonical splice cite'}, inplace=True)\n",
    "\n",
    "cutoff = 1\n",
    "n_pos: int = len(df.loc[df['Priority Score'] >= cutoff])\n",
    "n_neg: int = len(df.loc[df['Priority Score'] < cutoff])\n",
    "print(f\"Screen Positive: {n_pos}, Screen Negative: {n_neg}\")\n",
    "\n",
    "def add_screening_result_col(x) -> str:\n",
    "    if x >= cutoff:\n",
    "        return f\"Positive (n = {n_pos})\"\n",
    "    else:\n",
    "        return f\"Negative (n = {n_neg})\"\n",
    "    \n",
    "# def change_boolen_to_str(x) -> str:\n",
    "#     if x == 'true':\n",
    "#         return 'eLoF gene'\n",
    "#     else:\n",
    "#         return 'Non-eLoF gene'\n",
    "    \n",
    "df['Screening Result'] = df['Priority Score'].apply(add_screening_result_col)\n",
    "# df['eLoF'] = df['eLoF'].replace({True: 'eLoF gene', False: 'Non-eLoF gene'})\n",
    "df['Canonical splice cite'] = df['Canonical splice cite'].replace({'True': 'Canonical', 'False': 'Non-canonical'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       False\n",
       "1        True\n",
       "3       False\n",
       "10      False\n",
       "11      False\n",
       "        ...  \n",
       "6409     True\n",
       "6411    False\n",
       "6414    False\n",
       "6415    False\n",
       "6417    False\n",
       "Name: eLoF, Length: 2640, dtype: bool"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['eLoF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unsolved cases: 38\n",
      "Number of known disease genes: 63\n",
      "Number of unknown disease genes: 75\n",
      "Total number of variants: 132\n"
     ]
    }
   ],
   "source": [
    "### Add Sample_IDs that have been solved to the true_list below \n",
    "true_list = ['Sample_20526', 'Sample_17367', 'Sample_5100', 'Sample_21599',\n",
    "            'Sample_11750', 'Sample_7528', 'Sample_2288', 'Sample_17367',\n",
    "            'Sample_11873', 'Sample_6024', 'Sample_21407', 'Sample_7605',\n",
    "            'Sample_11467', 'Sample_17483', 'Sample_8021', 'Sample_5037',\n",
    "            'Sample_12927', 'Sample_20526', 'Sample_17367', 'Sample_22460',\n",
    "            'Sample_13765', 'Sample_52', 'Sample_5766', 'Sample_16260',\n",
    "            'Sample_5766', 'Sample_3986', 'Sample_13920', 'Sample_22025',\n",
    "            'Sample_13635', 'Sample_7723', 'Sample_19560', 'Sample_8928',\n",
    "            'Sample_17579', 'Sample_20287', 'Sample_12988', 'Sample_9869', \n",
    "            'Sample_20078', 'Sample_21789', 'Sample_21156', 'Sample_19227', \n",
    "            'Sample_17367', 'Sample_14452', 'Sample_11444', 'Sample_10713', \n",
    "            'Sample_9091', 'Sample_8436', 'Sample_4752', 'Sample_372', \n",
    "            'Sample_20468', 'Sample_9043', 'Sample_6024', 'Sample_21206',\n",
    "            'Sample_19880', 'Sample_13387', 'Sample_12988', 'Sample_12291', \n",
    "            'Sample_11555', 'Sample_4938', 'Sample_4413', 'Sample_2325']\n",
    "\n",
    "df = df.loc[df['maxsplai'] != 'NA']\n",
    "df = df.astype({'maxsplai': float})\n",
    "\n",
    "# Extract above samples from dataframe\n",
    "df = df.loc[df['sample'].isin(true_list)]\n",
    "\n",
    "for s in list(df.loc[df['Screening Result'] == f\"Positive (n = {n_pos})\", 'sample']):\n",
    "    if s not in set(true_list):\n",
    "        print(s)\n",
    "\n",
    "\n",
    "# total number of unsoloved cases\n",
    "print(f\"Total number of unsolved cases: {len(df['sample'].unique().tolist())}\")\n",
    "\n",
    "df.fillna({'DM': 0}, inplace=True)\n",
    "df.loc[df['DM'].isnull(), 'is_known_disease_gene'] = False\n",
    "df.loc[df['DM'] == 0, 'is_known_disease_gene'] = False\n",
    "df.loc[df['DM'] >= 1, 'is_known_disease_gene'] = True\n",
    "num_known_disease_genes: int = len(df.loc[df['is_known_disease_gene'] == True])\n",
    "num_unknown_disease_genes: int = len(df.loc[df['is_known_disease_gene'] == False])\n",
    "print(f\"Number of known disease genes: {num_known_disease_genes}\")\n",
    "print(f\"Number of unknown disease genes: {num_unknown_disease_genes}\")\n",
    "\n",
    "df.loc[df['is_known_disease_gene'] == True, 'Known disease gene'] = \"Known\"\n",
    "df.loc[df['is_known_disease_gene'] == False, 'Known disease gene'] = \"Unknown\"\n",
    "\n",
    "# Drop duplicates in the dataframe by 'sample' and 'variant_id'\n",
    "df = df.drop_duplicates(subset=['sample', 'variant_id'])\n",
    "\n",
    "# Sort by maxsplai\n",
    "df = df.loc[df['maxsplai'] != 'NA']\n",
    "df = df.loc[df['Priority Score'] != None]\n",
    "df = df.sort_values(by='maxsplai', ascending=True)\n",
    "print(f\"Total number of variants: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae066113f6042a18f6c5044ae7257a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'marker': {'color': 'gray'},\n",
       "              'mode': 'markers',\n",
       "              'selected': {'marker': {'color': 'firebrick'}},\n",
       "              'type': 'scatter',\n",
       "              'uid': '16c10c85-970d-4a26-8ca7-609da166beac',\n",
       "              'unselected': {'marker': {'opacity': 0.6}},\n",
       "              'x': array([-4., -3., -3., -3., -4., -9., -3., -4., -3., -4., -3., -4., -3., -3.,\n",
       "                          -3., -4., -3., -3., -3., -3., -4., -4.,  4., -3., -3., -3., -3., -3.,\n",
       "                          -3., -3., -3., -3., -3., -4., -3., -3., -4., -3., -3., -9., -3., -3.,\n",
       "                          -3., -3., -3., -4., -4., -4., -4., -3., -3., -4., -3., -4., -3., -4.,\n",
       "                          -4., -3., -3., -4., -3., -3., -4.,  1., -3., -4., -3., -3., -3., -3.,\n",
       "                          -3., -3., -3., -3., -4., -4., -3., -9., -3., -3., -3., -3., -4., -3.,\n",
       "                          -4., -3., -4., -3., -3., -4., -3., -3.,  0.,  0.,  7.,  2.,  2.,  9.,\n",
       "                           3.,  3.,  2.,  7.,  2.,  3.,  3.,  7.,  7.,  3.,  3.,  3.,  4.,  2.,\n",
       "                           3.,  3.,  3.,  3.,  7.,  3.,  3., 12.,  3.,  5.,  3.,  3.,  3.,  5.,\n",
       "                           5.,  3.,  3.,  3.,  3.,  3.]),\n",
       "              'y': array([0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                          0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                          0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.01, 0.01,\n",
       "                          0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                          0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.02, 0.02, 0.02, 0.02,\n",
       "                          0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                          0.03, 0.03, 0.03, 0.03, 0.03, 0.04, 0.04, 0.04, 0.04, 0.05, 0.05, 0.05,\n",
       "                          0.05, 0.05, 0.06, 0.08, 0.1 , 0.1 , 0.1 , 0.1 , 0.12, 0.15, 0.2 , 0.2 ,\n",
       "                          0.21, 0.5 , 0.66, 0.68, 0.68, 0.68, 0.75, 0.85, 0.9 , 0.91, 0.91, 0.94,\n",
       "                          0.95, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.99, 0.99, 0.99, 0.99, 0.99,\n",
       "                          0.99, 0.99, 0.99, 0.99, 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  ])},\n",
       "             {'dimensions': [{'label': 'Canonical splice cite',\n",
       "                              'values': array(['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No',\n",
       "                                               'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No',\n",
       "                                               'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No',\n",
       "                                               'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No',\n",
       "                                               'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No',\n",
       "                                               'No', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No',\n",
       "                                               'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No',\n",
       "                                               'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No',\n",
       "                                               'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'No', 'No',\n",
       "                                               'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'No',\n",
       "                                               'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes',\n",
       "                                               'Yes', 'Yes', 'Yes', 'Yes', 'Yes'], dtype=object)},\n",
       "                             {'label': 'eLoF',\n",
       "                              'values': array(['Non-eLoF gene', 'Non-eLoF gene', 'Non-eLoF gene', 'Non-eLoF gene',\n",
       "                                               'eLoF gene', 'eLoF gene', 'Non-eLoF gene', 'eLoF gene', 'Non-eLoF gene',\n",
       "                                               'Non-eLoF gene', 'Non-eLoF gene', 'Non-eLoF gene', 'eLoF gene',\n",
       "                                               'Non-eLoF gene', 'Non-eLoF gene', 'Non-eLoF gene', 'Non-eLoF gene',\n",
       "                                               'Non-eLoF gene', 'Non-eLoF gene', 'eLoF gene', 'Non-eLoF gene',\n",
       "                                               'Non-eLoF gene', 'eLoF gene', 'Non-eLoF gene', 'eLoF gene',\n",
       "                                               'Non-eLoF gene', 'Non-eLoF gene', 'Non-eLoF gene', 'Non-eLoF gene',\n",
       "                                               'Non-eLoF gene', 'Non-eLoF gene', 'eLoF gene', 'Non-eLoF gene',\n",
       "                                               'Non-eLoF gene', 'Non-eLoF gene', 'eLoF gene', 'Non-eLoF gene',\n",
       "                                               'Non-eLoF gene', 'Non-eLoF gene', 'eLoF gene', 'Non-eLoF gene',\n",
       "                                               'Non-eLoF gene', 'Non-eLoF gene', 'Non-eLoF gene', 'Non-eLoF gene',\n",
       "                                               'Non-eLoF gene', 'eLoF gene', 'eLoF gene', 'Non-eLoF gene', 'eLoF gene',\n",
       "                                               'Non-eLoF gene', 'eLoF gene', 'Non-eLoF gene', 'Non-eLoF gene',\n",
       "                                               'Non-eLoF gene', 'Non-eLoF gene', 'eLoF gene', 'Non-eLoF gene',\n",
       "                                               'eLoF gene', 'Non-eLoF gene', 'eLoF gene', 'eLoF gene', 'Non-eLoF gene',\n",
       "                                               'Non-eLoF gene', 'Non-eLoF gene', 'Non-eLoF gene', 'eLoF gene',\n",
       "                                               'Non-eLoF gene', 'Non-eLoF gene', 'Non-eLoF gene', 'Non-eLoF gene',\n",
       "                                               'Non-eLoF gene', 'Non-eLoF gene', 'Non-eLoF gene', 'Non-eLoF gene',\n",
       "                                               'Non-eLoF gene', 'Non-eLoF gene', 'eLoF gene', 'Non-eLoF gene',\n",
       "                                               'Non-eLoF gene', 'Non-eLoF gene', 'eLoF gene', 'Non-eLoF gene',\n",
       "                                               'Non-eLoF gene', 'eLoF gene', 'eLoF gene', 'Non-eLoF gene', 'eLoF gene',\n",
       "                                               'eLoF gene', 'Non-eLoF gene', 'Non-eLoF gene', 'eLoF gene', 'eLoF gene',\n",
       "                                               'eLoF gene', 'Non-eLoF gene', 'eLoF gene', 'eLoF gene', 'eLoF gene',\n",
       "                                               'Non-eLoF gene', 'Non-eLoF gene', 'Non-eLoF gene', 'eLoF gene',\n",
       "                                               'eLoF gene', 'Non-eLoF gene', 'Non-eLoF gene', 'eLoF gene', 'eLoF gene',\n",
       "                                               'Non-eLoF gene', 'Non-eLoF gene', 'Non-eLoF gene', 'Non-eLoF gene',\n",
       "                                               'Non-eLoF gene', 'Non-eLoF gene', 'Non-eLoF gene', 'Non-eLoF gene',\n",
       "                                               'Non-eLoF gene', 'eLoF gene', 'Non-eLoF gene', 'Non-eLoF gene',\n",
       "                                               'eLoF gene', 'Non-eLoF gene', 'eLoF gene', 'Non-eLoF gene',\n",
       "                                               'Non-eLoF gene', 'Non-eLoF gene', 'eLoF gene', 'eLoF gene',\n",
       "                                               'Non-eLoF gene', 'eLoF gene', 'Non-eLoF gene', 'Non-eLoF gene',\n",
       "                                               'Non-eLoF gene'], dtype=object)},\n",
       "                             {'label': 'Priority Score',\n",
       "                              'values': array([-4., -3., -3., -3., -4., -9., -3., -4., -3., -4., -3., -4., -3., -3.,\n",
       "                                               -3., -4., -3., -3., -3., -3., -4., -4.,  4., -3., -3., -3., -3., -3.,\n",
       "                                               -3., -3., -3., -3., -3., -4., -3., -3., -4., -3., -3., -9., -3., -3.,\n",
       "                                               -3., -3., -3., -4., -4., -4., -4., -3., -3., -4., -3., -4., -3., -4.,\n",
       "                                               -4., -3., -3., -4., -3., -3., -4.,  1., -3., -4., -3., -3., -3., -3.,\n",
       "                                               -3., -3., -3., -3., -4., -4., -3., -9., -3., -3., -3., -3., -4., -3.,\n",
       "                                               -4., -3., -4., -3., -3., -4., -3., -3.,  0.,  0.,  7.,  2.,  2.,  9.,\n",
       "                                                3.,  3.,  2.,  7.,  2.,  3.,  3.,  7.,  7.,  3.,  3.,  3.,  4.,  2.,\n",
       "                                                3.,  3.,  3.,  3.,  7.,  3.,  3., 12.,  3.,  5.,  3.,  3.,  3.,  5.,\n",
       "                                                5.,  3.,  3.,  3.,  3.,  3.])},\n",
       "                             {'label': 'Screening Result',\n",
       "                              'values': array(['Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Positive (n = 237)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Positive (n = 237)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Negative (n = 2401)', 'Negative (n = 2401)',\n",
       "                                               'Negative (n = 2401)', 'Positive (n = 237)', 'Positive (n = 237)',\n",
       "                                               'Positive (n = 237)', 'Positive (n = 237)', 'Positive (n = 237)',\n",
       "                                               'Positive (n = 237)', 'Positive (n = 237)', 'Positive (n = 237)',\n",
       "                                               'Positive (n = 237)', 'Positive (n = 237)', 'Positive (n = 237)',\n",
       "                                               'Positive (n = 237)', 'Positive (n = 237)', 'Positive (n = 237)',\n",
       "                                               'Positive (n = 237)', 'Positive (n = 237)', 'Positive (n = 237)',\n",
       "                                               'Positive (n = 237)', 'Positive (n = 237)', 'Positive (n = 237)',\n",
       "                                               'Positive (n = 237)', 'Positive (n = 237)', 'Positive (n = 237)',\n",
       "                                               'Positive (n = 237)', 'Positive (n = 237)', 'Positive (n = 237)',\n",
       "                                               'Positive (n = 237)', 'Positive (n = 237)', 'Positive (n = 237)',\n",
       "                                               'Positive (n = 237)', 'Positive (n = 237)', 'Positive (n = 237)',\n",
       "                                               'Positive (n = 237)', 'Positive (n = 237)', 'Positive (n = 237)',\n",
       "                                               'Positive (n = 237)', 'Positive (n = 237)', 'Positive (n = 237)'],\n",
       "                                              dtype=object)}],\n",
       "              'domain': {'y': [0, 0.5]},\n",
       "              'line': {'cmax': 1,\n",
       "                       'cmin': 0,\n",
       "                       'color': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8),\n",
       "                       'colorscale': [[0, 'lightgray'], [1, 'red']],\n",
       "                       'shape': 'hspline'},\n",
       "              'type': 'parcats',\n",
       "              'uid': 'f86b9fe0-d682-4e17-9210-72d00efa7088'}],\n",
       "    'layout': {'dragmode': 'lasso',\n",
       "               'height': 800,\n",
       "               'hovermode': 'closest',\n",
       "               'template': '...',\n",
       "               'width': 1000,\n",
       "               'xaxis': {'tickvals': [-9, -8, -7, -6, -5, -4, -3, -3, -2, -1, 0,\n",
       "                                      1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       "                         'title': {'text': 'Priority Score'}},\n",
       "               'yaxis': {'domain': [0.6, 1],\n",
       "                         'range': [-0.05, 1.05],\n",
       "                         'tickmode': 'array',\n",
       "                         'ticktext': [0.0, 0.25, 0.50, 0.75, 1.0],\n",
       "                         'tickvals': [0, 0.25, 0.5, 0.75, 1],\n",
       "                         'title': {'text': 'Maximum SpliceAI ∆score'}}}\n",
       "})"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "from ipywidgets import widgets\n",
    "\n",
    "# dimensions = [dict(values=cars_df[label], label=label) for label in categorical_dimensions]\n",
    "categorical_dimensions = [\"Canonical splice cite\", \"eLoF\", \"Priority Score\", \"Screening Result\"]\n",
    "dimensions = [dict(values=df[label], label=label) for label in categorical_dimensions]\n",
    "\n",
    "# Build colorscale\n",
    "color = np.zeros(len(df), dtype='uint8')\n",
    "colorscale = [[0, 'lightgray'], [1, 'red']]\n",
    "\n",
    "# Build figure as FigureWidget\n",
    "fig = go.FigureWidget(\n",
    "    data=[\n",
    "        go.Scatter(\n",
    "            x=df['Priority Score'], y=df['maxsplai'],\n",
    "            marker={'color': 'gray'}, mode='markers', selected={'marker': {'color': 'firebrick'}},\n",
    "            unselected={'marker': {'opacity': 0.6}}), \n",
    "        go.Parcats(\n",
    "            domain={'y': [0, 0.5]}, \n",
    "            dimensions=dimensions,\n",
    "            line={'colorscale': colorscale, 'cmin': 0,'cmax': 1, 'color': color, 'shape': 'hspline'})\n",
    "    ])\n",
    "\n",
    "fig.update_layout(\n",
    "        height=800, \n",
    "        xaxis={'title': 'Priority Score', \n",
    "               'tickvals': [-9, -8, -7, -6, -5, -4, -3, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]},\n",
    "        yaxis={'title': 'Maximum SpliceAI ∆score', \n",
    "               'domain': [0.6, 1], \n",
    "               'range': [-0.05, 1.05], \n",
    "               'tickmode': 'array',\n",
    "               'tickvals': [0, 0.25, 0.5, 0.75, 1], \n",
    "               'ticktext': ['0.0', '0.25', '0.50', '0.75', '1.0']},\n",
    "        dragmode='lasso', hovermode='closest')\n",
    "\n",
    "def update_color(trace, points, state):\n",
    "    # Update scatter selection\n",
    "    fig.data[0].selectedpoints = points.point_inds\n",
    "\n",
    "    # Update parcats colors\n",
    "    new_color = np.zeros(len(df), dtype='uint8')\n",
    "    new_color[points.point_inds] = 1\n",
    "    fig.data[1].line.color = new_color\n",
    "\n",
    "# Register callback on scatter selection...\n",
    "fig.data[0].on_selection(update_color)\n",
    "# and parcats click\n",
    "fig.data[1].on_click(update_color)\n",
    "\n",
    "# Update fig size\n",
    "fig.update_layout(width=1000, height=800)\n",
    "\n",
    "# Save as html\n",
    "# fig.write_html('FigureS3.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variants in known disease genes: 20\n"
     ]
    }
   ],
   "source": [
    "df = df.loc[df['Priority Score'] >=cutoff]\n",
    "df2 = df\n",
    "df2 = df2.rename(columns={'sample': 'Case'})\n",
    "\n",
    "df2['Removed'] = df2['Case'].apply(lambda x: 'Removed' if x not in set(df['sample']) else 'Remain')\n",
    "df2 = df2.loc[df2['Removed'] == 'Remain']\n",
    "df2_known = df2.loc[df2['is_known_disease_gene'] == True].copy()\n",
    "print(f\"Number of variants in known disease genes: {len(df2_known)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add interpriation\n",
    "interpritations: dict = {\n",
    "\t\"Sample_20526\": \"Mismatched phenotype\",\n",
    "\t\"Sample_7605\": \t\"Mismatched phenotype\",\n",
    "\t\"Sample_22025\": \"Mismatched phenotype\",\n",
    "\t\"Sample_8928\": \t\"Mismatched phenotype\",\n",
    "\t\"Sample_16260\": \"Mismatched phenotype\",\n",
    "\t\"Sample_21407\": \"P (Reported variant)\",\n",
    "\t\"Sample_11467\": \"P (Reported variant)\",\n",
    "\t\"Sample_5100\": \t\"Mismatched phenotype\",\n",
    "\t\"Sample_21599\": \"LP (Novel variant)\",\n",
    "\t\"Sample_19560\": \"Mismatched inheritance\",\n",
    "\t\"Sample_13635\": \"Mismatched inheritance\",\n",
    "\t\"Sample_5037\": \t\"Mismatched phenotype\",\n",
    "\t\"Sample_12927\": \"Mismatched phenotype\",\n",
    "\t\"Sample_7528\": \t\"Mismatched phenotype\",\n",
    "\t\"Sample_2288\": \t\"Mismatched phenotype\",\n",
    "\t\"Sample_17579\": \"Mismatched phenotype\",\n",
    "\t\"Sample_7723\": \t\"Mismatched phenotype\",\n",
    "\t\"Sample_22460\": \"Mismatched phenotype\",\n",
    "\t\"Sample_17483\": \"P (Novel variant)\",\n",
    "\t\"Sample_11873\": \"Mismatched phenotype\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_interpritation(x: str) -> str:\n",
    "\tif x in interpritations.keys():\n",
    "\t\treturn interpritations[x]\n",
    "\n",
    "df2_known['Interpretation'] = df2_known['Case'].apply(add_interpritation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_unkown = df2.loc[df2['is_known_disease_gene'] == False].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_known.to_excel('excel/positive_known_fixed.xlsx', index=False)\n",
    "df2_unkown.to_excel('excel/positive_unknown_fixed.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6328    2.0\n",
       "Name: Priority Score, dtype: float64"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_known.loc[df2_known['GeneSymbol'] == 'PDHA1', 'Priority Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Final interpretation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/sss/lib/python3.8/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniforge3/envs/sss/lib/python3.8/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/sss/lib/python3.8/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Final interpretation'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[306], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m n_knwon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df2\u001b[38;5;241m.\u001b[39mloc[df2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKnown disease gene\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mknown\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      5\u001b[0m n_unknown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df2\u001b[38;5;241m.\u001b[39mloc[df2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKnown disease gene\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 7\u001b[0m n_novel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df2\u001b[38;5;241m.\u001b[39mloc[\u001b[43mdf2\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFinal interpretation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNovel candidate gene\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      8\u001b[0m n_vus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df2\u001b[38;5;241m.\u001b[39mloc[df2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal interpretation\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVUS\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      9\u001b[0m n_phenomismatch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df2\u001b[38;5;241m.\u001b[39mloc[df2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal interpretation\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPhenotype mismatch\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniforge3/envs/sss/lib/python3.8/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniforge3/envs/sss/lib/python3.8/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Final interpretation'"
     ]
    }
   ],
   "source": [
    "# counting\n",
    "# known_txt = f\"Known (n = {num_known_disease_genes})\"\n",
    "# unknown_txt = f\"Unknown (n = {num_unknown_disease_genes})\"\n",
    "n_knwon = len(df2.loc[df2['Known disease gene'] == \"known\"])\n",
    "n_unknown = len(df2.loc[df2['Known disease gene'] == \"Unknown\"])\n",
    "\n",
    "n_novel = len(df2.loc[df2['Final interpretation'] == 'Novel candidate gene'])\n",
    "n_vus = len(df2.loc[df2['Final interpretation'] == 'VUS'])\n",
    "n_phenomismatch = len(df2.loc[df2['Final interpretation'] == 'Phenotype mismatch'])\n",
    "n_lp = len(df2.loc[df2['Final interpretation'] == 'LP (Novel)'])\n",
    "n_p_reported = len(df2.loc[df2['Final interpretation'] == 'P (Reported)'])\n",
    "n_p_novel = len(df2.loc[df2['Final interpretation'] == 'P (Novel)'])\n",
    "n_inhmismatch = len(df2.loc[df2['Final interpretation'] == 'Inheritance mismatch'])\n",
    "\n",
    "def add_num_to_gene_class(x) -> str:\n",
    "    if x == 'Known':\n",
    "        return f\"Known <br>(n = {n_knwon})\"\n",
    "    else:\n",
    "        return f\"Unknown <br>(n = {n_unknown})\"\n",
    "\n",
    "def add_num_to_interpretation(x) -> str:\n",
    "    if x == 'Novel candidate gene':\n",
    "        return f\"Novel candidate gene (n = {n_novel})\"\n",
    "    elif x == 'VUS':\n",
    "        return f\"VUS (n = {n_vus})\"\n",
    "    elif x == 'Phenotype mismatch':\n",
    "        return f\"Mismatched phenotype (n = {n_phenomismatch})\"\n",
    "    elif x == 'LP (Novel)':\n",
    "        return f\"LP (Novel) (n = {n_lp})\"\n",
    "    elif x == 'P (Reported)':\n",
    "        return f\"P (Reported) (n = {n_p_reported})\"\n",
    "    elif x == 'P (Novel)':\n",
    "        return f\"P (Novel) (n = {n_p_novel})\"\n",
    "    elif x == 'Inheritance mismatch':\n",
    "        return f\"Mismatched inheritance (n = {n_inhmismatch})\"\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "# df2['Known disease gene'] = df2['Known disease gene'].apply(add_num_to_gene_class)\n",
    "df2['Final interpretation'] = df2['Final interpretation'].apply(add_num_to_interpretation)    \n",
    "\n",
    "categorical_dimensions = [\"Known disease gene\", \"Reported inheritance\", \"eLoF\", \"Priority Score\", \"Final interpretation\"]\n",
    "dimensions = [dict(values=df2[label], label=label) for label in categorical_dimensions]\n",
    "\n",
    "# Build colorscale\n",
    "color = np.zeros(len(df), dtype='uint8')\n",
    "colorscale = [[0, 'lightgray'], [1, 'red']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
