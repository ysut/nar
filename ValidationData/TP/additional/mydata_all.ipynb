{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda create -n sss python=3.8 -y && conda activate sss\n",
    "# conda install -y -c bioconda gffutils jupyter tqdm cyvcf2 pathlib2 pandarallel pysam liftover pybedtools\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from Bio.Seq import Seq\n",
    "# from liftover import get_lifter\n",
    "from pathlib2 import Path\n",
    "from pandarallel import pandarallel\n",
    "from tqdm import tqdm\n",
    "import gffutils\n",
    "import pysam\n",
    "from cyvcf2 import VCF\n",
    "\n",
    "### Logging setup\n",
    "from logging import getLogger, config\n",
    "import yaml\n",
    "parent_directory = os.path.dirname(os.path.dirname('__file__'))\n",
    "config_path: str = os.path.join(parent_directory, '../../../config/logging.yaml')\n",
    "with open(config_path, 'r') as f:\n",
    "    config.dictConfig(yaml.safe_load(f))\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "########   Initialize and setup pandas methods   ########\n",
    "os.environ['JOBLIB_TEMP_FOLDER'] = '/tmp' \n",
    "pandarallel.initialize(nb_workers=5, progress_bar=True, verbose=1, use_memory_fs=False) \n",
    "tqdm.pandas()\n",
    "\n",
    "import sys\n",
    "try: \n",
    "    __file__\n",
    "    sys.path.append(os.path.join(os.path.dirname('__file__')))\n",
    "except NameError:\n",
    "    Path().resolve()\n",
    "    sys.path.append(os.path.join(Path().resolve(), '../../../'))\n",
    "\n",
    "from libs import utils, preprocess, variantfilter, posparser, splaiparser\n",
    "# from libs import predeffect, scoring\n",
    "from libs import anno_spliceai, anno_clinvar\n",
    "from libs.deco import print_filtering_count\n",
    "# from libs import predeffect\n",
    "from libs.scoring import Scoring\n",
    "from libs import predeffect\n",
    "\n",
    "\n",
    "gencode_gff = '../../../Resources/05_GENCODE_v43lift37/gencode.v43lift37.annotation.sort.gff3.gz'\n",
    "\n",
    "try:\n",
    "    db_anno_gencode = '../../../Resources/06_gffutilsdb/gencode.v43lift37.annotation.gtf.db'\n",
    "    db_anno_intron = '../../../Resources/06_gffutilsdb/gencode.v43lift37.annotation.intron.gtf.db'\n",
    "    db = gffutils.FeatureDB(db_anno_gencode)\n",
    "    db_intron = gffutils.FeatureDB(db_anno_intron)\n",
    "except ValueError:\n",
    "    db_anno_gencode = '/resources/DBs/gencode.v43lift37.annotation.gtf.db'\n",
    "    db_anno_intron = '/resources/DBs/gencode.v43lift37.annotation.intron.gtf.db'\n",
    "    db = gffutils.FeatureDB(db_anno_gencode)\n",
    "    db_intron = gffutils.FeatureDB(db_anno_intron)\n",
    "\n",
    "## Thresholds configuration\n",
    "thresholds_SpliceAI_parser: dict = {\n",
    "    'TH_min_sALDL': 0.02, 'TH_max_sALDL': 0.2, \n",
    "    'TH_min_sAGDG': 0.01, 'TH_max_sAGDG': 0.05,\n",
    "    'TH_min_GExon': 25, 'TH_max_GExon': 500,\n",
    "    'TH_sAG': 0.2, 'TH_sDG': 0.2\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解析前の raw vcf ファイルのパスは後でまた使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_vcf: str = \"/Users/utsu/work/Github/nar/mydata/mydata.splai.pangolin.vep.maxentscan.loftee.vcf\"\n",
    "fp = Path(raw_vcf)\n",
    "fp_stem, fp_dir = fp.stem, fp.parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse VCF and annotate ENST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Parse VCF to simple input table\n",
    "vcf = VCF(raw_vcf)\n",
    "header = vcf.header_iter()\n",
    "for h in header:\n",
    "    try:\n",
    "        h['ID']\n",
    "    except KeyError:\n",
    "        continue\n",
    "    else:\n",
    "        if h['ID'] == 'CSQ':\n",
    "            vep_cols_list = h['Description'].split('Format: ')[1].rstrip('\"').split('|')\n",
    "        elif h['ID'] == 'SpliceAI':\n",
    "            splai_cols_list = h['Description'].split('Format: ')[1].rstrip('\"').split('|')\n",
    "        elif h['ID'] == 'Pangolin':\n",
    "            pang_cols_list = h['Description'].split('Format: ')[1].rstrip('\"').split('|')\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "vepidx: dict = {col: i for i, col in enumerate(vep_cols_list)}\n",
    "splaidx: dict = {col: i for i, col in enumerate(splai_cols_list)}\n",
    "\n",
    "cols = [\n",
    "    'CHROM', 'POS', 'REF', 'ALT', 'GeneSymbol', 'SymbolSource', 'HGNC_ID', \n",
    "    'ENST', 'HGVSc', 'Consequence', 'EXON', 'INTRON', 'Strand',\n",
    "    'DS_AG', 'DS_AL', 'DS_DG', 'DS_DL', \n",
    "    'DP_AG', 'DP_AL', 'DP_DG', 'DP_DL', 'maxsplai',\n",
    "    'loftee', 'maxentscan_alt', 'maxentscan_diff', 'maxentscan_ref', \n",
    "    'pang_gene', 'pang_pos_socre_gain', 'pang_pos_score_loss', 'pang_warning', \n",
    "    'maxpangolin'\n",
    "]\n",
    "\n",
    "df: pd.DataFrame = pd.DataFrame(columns=cols)\n",
    "for v in VCF(raw_vcf):\n",
    "    vep: list = v.INFO.get('CSQ').split('|')\n",
    "\n",
    "    # Get HGVSc from VEP\n",
    "    try:\n",
    "        hgvsc = re.search('(?<=:).*',vep[vepidx['HGVSc']])[0]\n",
    "    except TypeError:\n",
    "        hgvsc = \"NA\"\n",
    "\n",
    "    # Get SpliceAI scores\n",
    "    if v.INFO.get('SpliceAI'):\n",
    "        splai: list = v.INFO.get('SpliceAI').split(',')[0].split('|')\n",
    "    else:\n",
    "        splai = ['NA'] * len(splai_cols_list)\n",
    "\n",
    "    # Get Pangolin scores\n",
    "    if v.INFO.get('Pangolin'):\n",
    "        pangolin: list = v.INFO.get('Pangolin').split('|')\n",
    "    else:\n",
    "        pangolin = ['NA'] * len(pang_cols_list)\n",
    "        \n",
    "    # Get Squirls scores\n",
    "    if v.INFO.get('SQUIRLS_SCORE'):\n",
    "        squirls: float = v.INFO.get('SQUIRLS_SCORE')\n",
    "    else:\n",
    "        squirls = \"NA\"\n",
    "\n",
    "    # Convert strand to +/- \n",
    "    strand = lambda s: '+' if s == '1' else '-'\n",
    "\n",
    "    # Get max SpliceAI scores\n",
    "    ds_ag: float = splai[splaidx['DS_AG']]\n",
    "    ds_al: float = splai[splaidx['DS_AL']]\n",
    "    ds_dg: float = splai[splaidx['DS_DG']]\n",
    "    ds_dl: float = splai[splaidx['DS_DL']]\n",
    "    if splai[splaidx['DP_AG']] == 'NA':\n",
    "        maxsplai: str = \"NA\"\n",
    "    maxsplai: float = max(ds_ag, ds_al, ds_dg, ds_dl)\n",
    "\n",
    "    # Get Pangplin scores\n",
    "    pang_gene: str = pangolin[0]\n",
    "    pang_pos_score_gain: str = pangolin[1]\n",
    "    pang_pos_score_loss: str = pangolin[2]\n",
    "    pang_warning: str = pangolin[3]\n",
    "    if pang_gene == 'NA':\n",
    "        maxpangolin = \"NA\"\n",
    "    else:\n",
    "        maxpangolin = max(\n",
    "            np.abs(float(pang_pos_score_gain.split(':')[1])), \n",
    "            np.abs(float(pang_pos_score_loss.split(':')[1]))\n",
    "            )\n",
    "\n",
    "    # Add df row\n",
    "    df = pd.concat([df, pd.DataFrame([[\n",
    "        v.CHROM, v.POS, v.REF, v.ALT[0], \n",
    "        vep[vepidx['SYMBOL']], vep[vepidx['SYMBOL_SOURCE']], \n",
    "        vep[vepidx['HGNC_ID']], vep[vepidx['Feature']], hgvsc, \n",
    "        vep[vepidx['Consequence']], \n",
    "        vep[vepidx['EXON']], vep[vepidx['INTRON']],\n",
    "        strand(vep[vepidx['STRAND']]), \n",
    "        ds_ag, ds_al, ds_dg, ds_dl,\n",
    "        splai[splaidx['DP_AG']], splai[splaidx['DP_AL']], \n",
    "        splai[splaidx['DP_DG']], splai[splaidx['DP_DL']],\n",
    "        maxsplai, vep[vepidx['LoF']], \n",
    "        vep[vepidx['MaxEntScan_alt']], \n",
    "        vep[vepidx['MaxEntScan_diff']], \n",
    "        vep[vepidx['MaxEntScan_ref']],\n",
    "        pang_gene, pang_pos_score_gain, pang_pos_score_loss, pang_warning,\n",
    "        maxpangolin]],\n",
    "        columns=cols)], ignore_index=True)\n",
    "        \n",
    "\n",
    "df['ENST_Full'] = df.apply(posparser.fetch_enst_full, db=db, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(f\"{fp_dir}/{fp_stem}.enst.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ここから解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/03/14 02:12:18 [INFO   ] (__main__) - Calculate the distance to the nearest splice site in intron variant...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16353/16353 [00:08<00:00, 1842.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/03/14 02:12:26 [INFO   ] (__main__) - Classify \"Canonical\" splice site or \"Non-canonical\" splice site...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 16353/16353 [00:06<00:00, 2709.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/03/14 02:12:33 [INFO   ] (__main__) - Annotating splicing information...\n",
      "2025/03/14 02:12:33 [INFO   ] (__main__) - Annotating ClinVar varaints interpretations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16353/16353 [00:00<00:00, 20679.45it/s]\n",
      "100%|██████████| 16353/16353 [00:00<00:00, 19091.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/03/14 02:12:35 [INFO   ] (__main__) - Parsing SpliceAI results...\n",
      "2025/03/14 02:12:35 [INFO   ] (__main__) - Annotating Exon/Intron position information...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16353/16353 [00:06<00:00, 2496.31it/s]\n",
      "100%|██████████| 16353/16353 [00:00<00:00, 18824.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/03/14 02:12:45 [INFO   ] (__main__) - Annotating aberrant splicing size (bp)...\n",
      "2025/03/14 02:12:45 [INFO   ] (__main__) - Predicting CDS change...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16353/16353 [00:04<00:00, 3516.30it/s]\n",
      "100%|██████████| 16353/16353 [00:00<00:00, 168212.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/03/14 02:12:51 [INFO   ] (__main__) - Annotating CCRs info...\n",
      "2025/03/14 02:12:51 [INFO   ] (__main__) - Annotate CCR score\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(f\"{fp_dir}/{fp_stem}.enst.pkl\")\n",
    "\n",
    "df = df.fillna(\n",
    "    {'loftee': '.', 'maxentscan_alt': '.', 'maxentscan_diff': '.', \n",
    "        'maxentscan_ref': '.', 'pang_gene': '.', 'pang_pos_socre_gain': '.',\n",
    "        'pang_pos_score_loss': '.', 'pang_warning': '.'}\n",
    "        )\n",
    "\n",
    "logger.info('Calculate the distance to the nearest splice site in intron variant...')\n",
    "df['IntronDist'] = df.progress_apply(\n",
    "    posparser.signed_distance_to_exon_boundary, \n",
    "    db=db, db_intron=db_intron, axis=1)\n",
    "\n",
    "logger.info('Classify \"Canonical\" splice site or \"Non-canonical\" splice site...')\n",
    "df = posparser.classifying_canonical(df)\n",
    "\n",
    "df['Ex_or_Int'] = np.where(\n",
    "    df['IntronDist'] == \"[Warning] Invalid ENST ID\", \"[Warning] Invalid ENST ID\",\n",
    "    np.where(df['IntronDist'].isnull(), 'Exonic', 'Intronic'))\n",
    "\n",
    "tbx_anno = pysam.TabixFile(gencode_gff)\n",
    "df['exon_loc'] = df.progress_apply(\n",
    "    posparser.calc_exon_loc, tabixfile=tbx_anno, enstcolname='ENST', axis=1)\n",
    "df = pd.concat([df, df['exon_loc'].str.split(':', expand=True)], axis=1)\n",
    "df.rename(columns={0: 'ex_up_dist', 1: 'ex_down_dist'}, inplace=True)\n",
    "df.drop(columns=['exon_loc'], inplace=True)\n",
    "\n",
    "#2-2. Select minimum distance from upstream distance and downstream distance\n",
    "df['exon_pos'] = df.parallel_apply(posparser.select_exon_pos, axis=1)\n",
    "#2-3. Relative exon location\n",
    "df['prc_exon_loc'] = df.parallel_apply(posparser.calc_prc_exon_loc, axis=1)\n",
    "\n",
    "#2-4. Decision exonic splice sites (1 nt in acceptor site or 3 nts on Donor site)\n",
    "df['exon_splice_site'] = df.parallel_apply(posparser.extract_splicing_region, axis=1)\n",
    "\n",
    "#3.   Additional Splicing information\n",
    "logger.info('Annotating splicing information...')\n",
    "#3-1. Annotate splicing type ('Exonic Acceptor' etc.)\n",
    "df['SpliceType'] = df.parallel_apply(posparser.select_donor_acceptor, axis=1)\n",
    "\n",
    "#5.   Annotate ClinVar varaints interpretations\n",
    "logger.info('Annotating ClinVar varaints interpretations...')\n",
    "clinvar_file = '../../../clinvar/Filtered_BCF_GRCh37_20241211-044124/clinvar_GRCh37.germline.nocoflicted.bcf.gz'\n",
    "cln_bcf = pysam.VariantFile(clinvar_file)\n",
    "df['clinvar_same_pos'] = df.progress_apply(\n",
    "    anno_clinvar.anno_same_pos_vars, cln_bcf=cln_bcf, axis=1)\n",
    "df['clinvar_same_motif'] = df.progress_apply(\n",
    "    anno_clinvar.anno_same_motif_vars, cln_bcf=cln_bcf, axis=1)\n",
    "df['same_motif_clinsigs'] = df['clinvar_same_motif'].parallel_apply(\n",
    "    anno_clinvar.extract_same_motif_clinsigs)\n",
    "\n",
    "logger.info('Parsing SpliceAI results...')\n",
    "logger.info('Annotating Exon/Intron position information...')\n",
    "df['ExInt_INFO'] = df.progress_apply(\n",
    "    splaiparser.calc_exint_info, db=db, db_intron=db_intron, axis=1)\n",
    "\n",
    "#6-3. Predict splicing effects\n",
    "df['Pseudoexon'] = df.progress_apply(\n",
    "    splaiparser.pseudoexon_activation,\n",
    "    thresholds=thresholds_SpliceAI_parser, \n",
    "    db_intron=db_intron,\n",
    "    axis=1)\n",
    "\n",
    "df['Part_IntRet'] = df.parallel_apply(\n",
    "    splaiparser.partial_intron_retention,\n",
    "    thresholds=thresholds_SpliceAI_parser, \n",
    "    axis=1)\n",
    "\n",
    "df['Part_ExDel'] = df.parallel_apply(\n",
    "    splaiparser.partial_exon_deletion,\n",
    "    thresholds=thresholds_SpliceAI_parser, \n",
    "    axis=1)\n",
    "\n",
    "df['Exon_skipping'] = df.parallel_apply(\n",
    "    splaiparser.exon_skipping, \n",
    "    thresholds=thresholds_SpliceAI_parser, \n",
    "    axis=1)\n",
    "                                        \n",
    "df['Int_Retention'] = df.parallel_apply(\n",
    "    splaiparser.intron_retention, \n",
    "    thresholds=thresholds_SpliceAI_parser, \n",
    "    axis=1)\n",
    "\n",
    "df['multiexs'] = df.parallel_apply(\n",
    "    splaiparser.multi_exon_skipping, \n",
    "    thresholds=thresholds_SpliceAI_parser, \n",
    "    axis=1)\n",
    "\n",
    "#7.   Annotate aberrant splicing size (bp)\n",
    "logger.info('Annotating aberrant splicing size (bp)...')\n",
    "#7-1. Annotate size of \n",
    "df['Size_Part_ExDel'] = df.parallel_apply(\n",
    "    splaiparser.anno_partial_exon_del_size, \n",
    "    thresholds=thresholds_SpliceAI_parser, \n",
    "    axis=1)\n",
    "\n",
    "#7-3. Annotate size of partial intron retention\n",
    "df['Size_Part_IntRet'] = df.parallel_apply(\n",
    "    splaiparser.anno_partial_intron_retention_size, \n",
    "    thresholds=thresholds_SpliceAI_parser,\n",
    "    axis=1)\n",
    "\n",
    "#7-2. Annotate size of pseudoexon\n",
    "df['Size_pseudoexon'] = df.parallel_apply(\n",
    "    splaiparser.anno_gained_exon_size, \n",
    "    thresholds=thresholds_SpliceAI_parser, \n",
    "    axis=1)\n",
    "\n",
    "#7-4. Annotate size of intron retention\n",
    "df['Size_IntRet'] = df.parallel_apply(\n",
    "    splaiparser.anno_intron_retention_size, \n",
    "    thresholds=thresholds_SpliceAI_parser,\n",
    "    axis=1)\n",
    "\n",
    "#7-5. Annotate size of exon skipping\n",
    "df['Size_skipped_exon'] = df.parallel_apply(\n",
    "    splaiparser.anno_skipped_exon_size, \n",
    "    thresholds=thresholds_SpliceAI_parser,\n",
    "    axis=1)\n",
    "\n",
    "df['variant_id'] = df['CHROM'].astype(str) + '-' \\\n",
    "    + df['POS'].astype(str) + '-' + df['REF'] + '-' + df['ALT']\n",
    "\n",
    "#8.   Evaluate splicing effects\n",
    "logger.info('Predicting CDS change...')\n",
    "#8-1. Predict CDS change\n",
    "df['CDS_Length'] = df.progress_apply(predeffect.calc_cds_len, db=db, axis=1)\n",
    "df['is_10%_truncation'] = df.progress_apply(predeffect.calc_cds_len_shorten, axis=1)\n",
    "\n",
    "#8-2. Determine if the gene is included in eLoFs genes\n",
    "df['is_eLoF'] = df.parallel_apply(predeffect.elofs_judge, axis=1)\n",
    "\n",
    "#8-3. Determine causing NMD or not\n",
    "df['is_NMD_at_Canon'] = df.parallel_apply(predeffect.nmd_judge, axis=1)\n",
    "\n",
    "#8-4. Frame check\n",
    "# Covert to str (Cannot predict splicing event) to np.nan\n",
    "cannot_predict: str = 'Cannot predict splicing event'\n",
    "df['Size_Part_ExDel'] = df['Size_Part_ExDel'].replace(cannot_predict, np.nan)\n",
    "df['Size_Part_IntRet'] = df['Size_Part_IntRet'].replace(cannot_predict, np.nan)\n",
    "df['Size_pseudoexon'] = df['Size_pseudoexon'].replace(cannot_predict, np.nan)\n",
    "df['Size_IntRet'] = df['Size_IntRet'].replace(cannot_predict, np.nan)\n",
    "df['Size_skipped_exon'] = df['Size_skipped_exon'].replace(cannot_predict, np.nan)\n",
    "\n",
    "df['is_Frameshift_Part_ExDel'] = df['Size_Part_ExDel'].parallel_apply(\n",
    "    predeffect.frame_check)\n",
    "df['is_Frameshift_Part_IntRet'] = df['Size_Part_IntRet'].parallel_apply(\n",
    "    predeffect.frame_check)\n",
    "df['is_Frameshift_pseudoexon'] = df['Size_pseudoexon'].parallel_apply(\n",
    "    predeffect.frame_check)\n",
    "df['is_Frameshift_IntRet'] = df['Size_IntRet'].parallel_apply(\n",
    "    predeffect.frame_check)\n",
    "df['is_Frameshift_skipped_exon'] = df['Size_skipped_exon'].parallel_apply(\n",
    "    predeffect.frame_check)\n",
    "df['is_Frameshift'] = df[['is_Frameshift_Part_ExDel', \n",
    "                        'is_Frameshift_Part_IntRet', \n",
    "                        'is_Frameshift_pseudoexon', \n",
    "                        'is_Frameshift_IntRet', \n",
    "                        'is_Frameshift_skipped_exon'\n",
    "                        ]].any(axis=1)\n",
    "\n",
    "#9.   CCRs\n",
    "logger.info('Annotating CCRs info...')\n",
    "#9-1. Annotate truncated regions \n",
    "df['skipped_region'] = df.parallel_apply(\n",
    "    splaiparser.anno_skipped_regions, axis=1)\n",
    "df['deleted_region'] = df.parallel_apply(\n",
    "    splaiparser.anno_deleted_regions, \n",
    "    thresholds=thresholds_SpliceAI_parser, axis=1)\n",
    "\n",
    "#9-2. Intersect with CCRs\n",
    "logger.info('Annotate CCR score')\n",
    "df = predeffect.anno_ccr_score(df)\n",
    "\n",
    "# Extract data with SymbolSource == 'HGNC'\n",
    "# df = df[df['SymbolSource'] == 'HGNC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(f\"{fp_dir}/{fp_stem}.enst.prescore.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### グラフ書くときはここから"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(f\"{fp_dir}/{fp_stem}.enst.prescore.pkl\")\n",
    "\n",
    "scoring = Scoring()\n",
    "df['insilico_screening'] = df.parallel_apply(scoring.insilico_screening, axis=1)\n",
    "df['clinvar_screening'] = df.parallel_apply(scoring.clinvar_screening, axis=1)\n",
    "df['recalibrated_splai'] = df.parallel_apply(scoring.recal_scores_in_canon, axis=1)\n",
    "\n",
    "def map_and_calc_score(row, score_map: dict) -> int:\n",
    "    \"\"\"\n",
    "    Map the score to the solution\n",
    "    s1, s2, s3, and s15 are clinvar_screening\n",
    "    s4, s5, s6, s7, s8, s9, s10 and s11 are insilico_screening\n",
    "    s12, s13 and s14 are recalibrated_splai\n",
    "    PriortiyScore is the sum of the \"clinvar_screening\", \"insilico_screening\", and \"recalibrated_splai\"\n",
    "    \"\"\"\n",
    "    if row['insilico_screening'] == \"Not available\":\n",
    "        return np.nan\n",
    "\n",
    "    return int(score_map[row['recalibrated_splai']]) + int(score_map[row['insilico_screening']]) + int(score_map[row['clinvar_screening']])\n",
    "\n",
    "solution = {'s1': 9.0, 's2': 6.0, 's3': 0.0, 's4': -5.0, \n",
    "            's5': -3.0, 's6': 0.0, 's7': 2.0, 's8': 3.0, 's9': 2.0,\n",
    "            's10': 4.0, 's11': 2.0, 's12': -1.0, 's13': 0.0, 's14': 1.0, \n",
    "            's15': -5.0, 's0': 0.0}\n",
    "\n",
    "df['PriorityScore'] = df.parallel_apply(map_and_calc_score, args=(solution,), axis=1)\n",
    "df.to_pickle(f\"{fp_dir}/{fp_stem}.enst.scored.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract screen positive variants only (PriorityScore >= 1)\n",
    "df = pd.read_pickle(f\"{fp_dir}/{fp_stem}.enst.scored.pkl\")\n",
    "df = df[df['PriorityScore'] >= 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De novo filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of variants: 16353\n",
      "Start extract_denovo\n",
      "Filtering : 16353 --> 9944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def anno_hgmd(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    path_to_hgmd = '../../../Resources/07_HGMD_GeneBasedInfo/HGMD_GeneBasedInfo_2024.1.tsv.gz'\n",
    "    hgmd = pd.read_table(path_to_hgmd, header=0, dtype=str)\n",
    "    hgmd = hgmd[['gene', 'altsymbol', 'refseq', \n",
    "                 'expected_inheritance', 'hgncID', 'omimid', 'DM']]\n",
    "    hgmd = hgmd.astype({'DM': 'float64'})\n",
    "    df = pd.merge(df, hgmd, left_on='HGNC_ID', right_on='hgncID', how='left')\n",
    "    return df\n",
    "\n",
    "def anno_sf(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_sf = pd.read_table('../../../Resources/ACMG_SFv3.2.txt', header=0, dtype=str)\n",
    "    df_sf = df_sf[['Gene', 'Disease/Phentyope', 'Inheritance ', 'Variants to report']]\n",
    "    df = pd.merge(df, df_sf, left_on='GeneSymbol', right_on='Gene', how='left')\n",
    "    return df\n",
    "\n",
    "mydata = './original.snpeff.state.disease.identifiedgene.filtered.splai.tsv'\n",
    "df = pd.read_table(mydata, sep='\\t', dtype=str)\n",
    "print(f\"Total number of variants: {len(df)}\")\n",
    "df = variantfilter.extract_denovo(df)\n",
    "df.loc[:,'is_denovo'] = True\n",
    "df = df[df['vqslod'] > -7.18]\n",
    "df = df[((df['denovogear'] > 0.02) | (df['denovogear'].isnull()))\n",
    "        & ((df['triodenovo'] > 5.72) | (df['triodenovo'].isnull()))\n",
    "        & ((df['dnmfilter'] > 0.196) | (df['dnmfilter'].isnull()))]\n",
    "df.drop(columns=['variant_id', 'ID_y'], inplace=True)\n",
    "df['variant_id'] = df['CHROM'] + '-' + df['POS'] + '-' + df['REF'] + '-' + df['ALT']\n",
    "\n",
    "# merge with mydata\n",
    "df_variant = pd.read_pickle(f\"{fp_dir}/{fp_stem}.enst.scored.pkl\")\n",
    "df_variant.drop_duplicates(subset=['variant_id'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variants after filtering: 6406\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'false_variants' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m df\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m~\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIdentified_Gene\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misnull(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIdentified\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# remove false_variants by variant_id key\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m~\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariant_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(\u001b[43mfalse_variants\u001b[49m)]\n\u001b[1;32m     34\u001b[0m df_solved \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIdentified\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     35\u001b[0m df_unsolved \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUndetermined\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'false_variants' is not defined"
     ]
    }
   ],
   "source": [
    "df = df[['sample', 'fa', 'mo', 'Disease', 'is_denovo', 'variant_id', 'State', 'Identified_Gene']]\n",
    "df = pd.merge(df, df_variant, on='variant_id')\n",
    "print(f\"Number of variants after filtering: {len(df)}\")\n",
    "\n",
    "df = anno_hgmd(df)\n",
    "df = anno_sf(df)\n",
    "df['skipped_ccrs'] = df['skipped_ccrs'].replace('.', np.nan).astype(float).copy()\n",
    "df['deleted_ccrs'] = df['deleted_ccrs'].replace('.', np.nan).astype(float)\n",
    "df['is_95%_CCRs'] = df.apply(lambda row: True if (row['skipped_ccrs'] > 95 or row['deleted_ccrs'] > 95) else False, axis=1)\n",
    "\n",
    "# Exclude error calling\n",
    "df = df.loc[df['variant_id'] != \"8-145138872-T-G\"]\n",
    "\n",
    "solved_case_ids = [\n",
    "    'Sample_4143', 'Sample_8803', 'Sample_17110', 'Sample_9768', 'Sample_16992',\n",
    "    'Sample_16970', 'Sample_4938', 'Sample_11555', 'Sample_10713']\n",
    "\n",
    "non_trios = [\n",
    "\t'Sample_17367', 'Sample_19880', 'Sample_7118', 'Sample_22831',\n",
    "\t'Sample_13784', 'Sample_8021', 'Sample_23636', 'Sample_5766', \n",
    "\t'Sample_12102', 'Sample_52', 'Sample_7700', 'Sample_3986', 'Sample_20591', \n",
    "\t'Sample_18910', 'Sample_11219', 'Sample_11895', 'Sample_10507', \n",
    "\t'Sample_14446', 'Sample_13089', 'Sample_2325', 'Sample_20287', \n",
    "\t'Sample_6024', 'Sample_16152', 'Sample_6584', 'Sample_10875', 'Sample_8436',\n",
    "\t'Sample_11750', 'Sample_13765', 'Sample_16783', 'Sample_15778']\n",
    "\n",
    "df = df.loc[~df['sample'].isin(non_trios)]\n",
    "df.loc[df['sample'].isin(solved_case_ids), 'State'] = 'Identified'\n",
    "df.loc[~df['Identified_Gene'].isnull(), 'State'] = 'Identified'\n",
    "\n",
    "# remove false_variants by variant_id key\n",
    "df = df.loc[~df['variant_id'].isin(false_variants)]\n",
    "\n",
    "df_solved = df[df['State'] == 'Identified']\n",
    "df_unsolved = df[df['State'] == 'Undetermined']\n",
    "\n",
    "print(f\"{len(df_solved)} + {len(df_unsolved)}\")\n",
    "print(len(df_unsolved['sample'].unique().tolist()))\n",
    "\n",
    "df = df.loc[df['State'] == 'Undetermined']\n",
    "df.rename(columns={'is_eLoF': 'eLoF', 'PriorityScore': 'Priority Score', \n",
    "                   'is_Canonical': 'Canonical splice cite'}, inplace=True)\n",
    "\n",
    "cutoff = 1\n",
    "n_pos: int = len(df.loc[df['Priority Score'] >= cutoff])\n",
    "n_neg: int = len(df.loc[df['Priority Score'] < cutoff])\n",
    "print(f\"Screen Positive: {n_pos}, Screen Negative: {n_neg}\")\n",
    "\n",
    "n_pos = 40\n",
    "n_neg = 2289\n",
    "\n",
    "\n",
    "def add_screening_result_col(x) -> str:\n",
    "    if x >= cutoff:\n",
    "        return f\"Positive (n = {n_pos})\"\n",
    "    else:\n",
    "        return f\"Negative (n = {n_neg})\"\n",
    "    \n",
    "# def change_boolen_to_str(x) -> str:\n",
    "#     if x == 'true':\n",
    "#         return 'eLoF gene'\n",
    "#     else:\n",
    "#         return 'Non-eLoF gene'\n",
    "    \n",
    "df['Screening Result'] = df['Priority Score'].apply(add_screening_result_col)\n",
    "# df['eLoF'] = df['eLoF'].replace({True: 'eLoF gene', False: 'Non-eLoF gene'})\n",
    "df['Canonical splice cite'] = df['Canonical splice cite'].replace({'True': 'Canonical', 'False': 'Non-canonical'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Screening Result'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/sss/lib/python3.8/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniforge3/envs/sss/lib/python3.8/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/sss/lib/python3.8/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Screening Result'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 29\u001b[0m\n\u001b[1;32m     22\u001b[0m df_false \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m~\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(true_list)]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Extract above samples from dataframe\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# print(len(df))\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# df = df.loc[df['sample'].isin(true_list)]\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# print(len(df))\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mloc[\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mScreening Result\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositive (n = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_pos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(true_list):\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28mprint\u001b[39m(s)\n",
      "File \u001b[0;32m~/miniforge3/envs/sss/lib/python3.8/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniforge3/envs/sss/lib/python3.8/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Screening Result'"
     ]
    }
   ],
   "source": [
    "### Add Sample_IDs that have been solved to the true_list below \n",
    "true_list = ['Sample_20526', 'Sample_17367', 'Sample_5100', 'Sample_21599',\n",
    "            'Sample_11750', 'Sample_7528', 'Sample_2288', 'Sample_17367',\n",
    "            'Sample_11873', 'Sample_6024', 'Sample_21407', 'Sample_7605',\n",
    "            'Sample_11467', 'Sample_17483', 'Sample_8021', 'Sample_5037',\n",
    "            'Sample_12927', 'Sample_20526', 'Sample_17367', 'Sample_22460',\n",
    "            'Sample_13765', 'Sample_52', 'Sample_5766', 'Sample_16260',\n",
    "            'Sample_5766', 'Sample_3986', 'Sample_13920', 'Sample_22025',\n",
    "            'Sample_13635', 'Sample_7723', 'Sample_19560', 'Sample_8928',\n",
    "            'Sample_17579', 'Sample_20287', 'Sample_12988', 'Sample_9869', \n",
    "            'Sample_20078', 'Sample_21789', 'Sample_21156', 'Sample_19227', \n",
    "            'Sample_17367', 'Sample_14452', 'Sample_11444', 'Sample_10713', \n",
    "            'Sample_9091', 'Sample_8436', 'Sample_4752', 'Sample_372', \n",
    "            'Sample_20468', 'Sample_9043', 'Sample_6024', 'Sample_21206',\n",
    "            'Sample_19880', 'Sample_13387', 'Sample_12988', 'Sample_12291', \n",
    "            'Sample_11555', 'Sample_4938', 'Sample_4413', 'Sample_2325']\n",
    "\n",
    "df = df.loc[df['maxsplai'] != 'NA']\n",
    "df = df.astype({'maxsplai': float})\n",
    "\n",
    "# Extract false samples from dataframe\n",
    "df_false = df.loc[~df['sample'].isin(true_list)]\n",
    "\n",
    "# Extract above samples from dataframe\n",
    "# print(len(df))\n",
    "# df = df.loc[df['sample'].isin(true_list)]\n",
    "# print(len(df))\n",
    "\n",
    "for s in list(df.loc[df['Screening Result'] == f\"Positive (n = {n_pos})\", 'sample']):\n",
    "    if s not in set(true_list):\n",
    "        print(s)\n",
    "\n",
    "# total number of unsoloved cases\n",
    "print(f\"Total number of unsolved cases: {len(df['sample'].unique().tolist())}\")\n",
    "\n",
    "df.fillna({'DM': 0}, inplace=True)\n",
    "df.loc[df['DM'].isnull(), 'is_known_disease_gene'] = False\n",
    "df.loc[df['DM'] == 0, 'is_known_disease_gene'] = False\n",
    "df.loc[df['DM'] >= 1, 'is_known_disease_gene'] = True\n",
    "num_known_disease_genes: int = len(df.loc[df['is_known_disease_gene'] == True])\n",
    "num_unknown_disease_genes: int = len(df.loc[df['is_known_disease_gene'] == False])\n",
    "print(f\"Number of known disease genes: {num_known_disease_genes}\")\n",
    "print(f\"Number of unknown disease genes: {num_unknown_disease_genes}\")\n",
    "\n",
    "df.loc[df['is_known_disease_gene'] == True, 'Known disease gene'] = \"Known\"\n",
    "df.loc[df['is_known_disease_gene'] == False, 'Known disease gene'] = \"Unknown\"\n",
    "\n",
    "# Drop duplicates in the dataframe by 'sample' and 'variant_id'\n",
    "df = df.drop_duplicates(subset=['sample', 'variant_id'])\n",
    "\n",
    "# Sort by maxsplai\n",
    "df = df.loc[df['maxsplai'] != 'NA']\n",
    "df = df.loc[df['Priority Score'] != None]\n",
    "df = df.sort_values(by='maxsplai', ascending=True)\n",
    "print(f\"Total number of variants: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "false_variants = df_false.loc[df_false['Screening Result'] == f\"Positive (n = {n_pos})\", 'variant_id'].unique().tolist()\n",
    "print(len(df.loc[df['Screening Result'] == f\"Positive (n = {n_pos})\", 'variant_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All revisited cases: 1257 cases\n"
     ]
    }
   ],
   "source": [
    "print(f\"All revisited cases: {len(df['sample'].unique().tolist())} cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70efd1ce83b3473ab02abf4f37fbc626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'marker': {'color': 'gray'},\n",
       "              'mode': 'markers',\n",
       "              'selected': {'marker': {'color': 'firebrick'}},\n",
       "              'type': 'scatter',\n",
       "              'uid': '759d13cc-7bfd-4867-9c73-99cb6b9c11eb',\n",
       "              'unselected': {'marker': {'opacity': 0.6}},\n",
       "              'x': array([-5., -3., -3., ...,  3.,  5.,  3.]),\n",
       "              'y': array([0., 0., 0., ..., 1., 1., 1.])},\n",
       "             {'dimensions': [{'label': 'Canonical splice cite',\n",
       "                              'values': array(['No', 'No', 'No', ..., 'Yes', 'Yes', 'Yes'], dtype=object)},\n",
       "                             {'label': 'eLoF',\n",
       "                              'values': array([False, True, True, ..., False, True, False], dtype=object)},\n",
       "                             {'label': 'Priority Score', 'values': array([-5., -3., -3., ...,  3.,  5.,  3.])},\n",
       "                             {'label': 'Screening Result',\n",
       "                              'values': array(['Negative (n = 2289)', 'Negative (n = 2289)', 'Negative (n = 2289)',\n",
       "                                               ..., 'Positive (n = 40)', 'Positive (n = 40)', 'Positive (n = 40)'],\n",
       "                                              dtype=object)}],\n",
       "              'domain': {'y': [0, 0.5]},\n",
       "              'line': {'cmax': 1,\n",
       "                       'cmin': 0,\n",
       "                       'color': array([0, 0, 0, ..., 0, 0, 0], dtype=uint8),\n",
       "                       'colorscale': [[0, 'lightgray'], [1, 'red']],\n",
       "                       'shape': 'hspline'},\n",
       "              'type': 'parcats',\n",
       "              'uid': '3bf168be-db4b-4a5d-9e55-6353151cdc8f'}],\n",
       "    'layout': {'dragmode': 'lasso',\n",
       "               'height': 800,\n",
       "               'hovermode': 'closest',\n",
       "               'template': '...',\n",
       "               'width': 1000,\n",
       "               'xaxis': {'range': [-12.5, 14.5],\n",
       "                         'tickvals': [-12, -11, -10, -9, -8, -7, -6, -5, -4, -3,\n",
       "                                      -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,\n",
       "                                      12, 13, 14],\n",
       "                         'title': {'text': 'Priority Score'}},\n",
       "               'yaxis': {'domain': [0.6, 1],\n",
       "                         'range': [-0.05, 1.05],\n",
       "                         'tickmode': 'array',\n",
       "                         'ticktext': [0.0, 0.20, 0.50, 0.80, 1.0],\n",
       "                         'tickvals': [0, 0.2, 0.5, 0.8, 1],\n",
       "                         'title': {'text': 'Maximum SpliceAI ∆score'}}}\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "from ipywidgets import widgets\n",
    "\n",
    "# dimensions = [dict(values=cars_df[label], label=label) for label in categorical_dimensions]\n",
    "categorical_dimensions = [\"Canonical splice cite\", \"eLoF\", \"Priority Score\", \"Screening Result\"]\n",
    "dimensions = [dict(values=df[label], label=label) for label in categorical_dimensions]\n",
    "\n",
    "# Build colorscale\n",
    "color = np.zeros(len(df), dtype='uint8')\n",
    "colorscale = [[0, 'lightgray'], [1, 'red']]\n",
    "\n",
    "# df = df.sort_values(by='Priority Score', ascending=True)\n",
    "\n",
    "# Default color\n",
    "# If Priority Score is greater than 1, the color is red in scatter plot\n",
    "color = np.zeros(len(df), dtype='uint8')\n",
    "# color[df['Priority Score'] >= 1] = 1\n",
    "\n",
    "# Build figure as FigureWidget\n",
    "fig = go.FigureWidget(\n",
    "    data=[\n",
    "        go.Scatter(\n",
    "            x=df['Priority Score'], y=df['maxsplai'],\n",
    "            marker={'color': 'gray'}, mode='markers', selected={'marker': {'color': 'firebrick'}},\n",
    "            unselected={'marker': {'opacity': 0.6}}), \n",
    "        go.Parcats(\n",
    "            domain={'y': [0, 0.5]}, \n",
    "            dimensions=dimensions,\n",
    "            line={'colorscale': colorscale, 'cmin': 0,'cmax': 1, 'color': color, 'shape': 'hspline'})\n",
    "    ])\n",
    "\n",
    "# fig.data[0].marker.color = color\n",
    "\n",
    "fig.update_layout(\n",
    "        height=800, \n",
    "        xaxis={'title': 'Priority Score', \n",
    "               'tickvals': list(range(-12, 15, 1)),\n",
    "               'range': [-12.5, 14.5]},\n",
    "        yaxis={'title': 'Maximum SpliceAI ∆score', \n",
    "               'domain': [0.6, 1], \n",
    "               'range': [-0.05, 1.05], \n",
    "               'tickmode': 'array',\n",
    "               'tickvals': [0, 0.2, 0.5, 0.8, 1], \n",
    "               'ticktext': ['0.0', '0.20', '0.50', '0.80', '1.0']},\n",
    "        dragmode='lasso', hovermode='closest')\n",
    "\n",
    "def update_color(trace, points, state):\n",
    "    # Update scatter selection\n",
    "    fig.data[0].selectedpoints = points.point_inds\n",
    "\n",
    "    # Update parcats colors\n",
    "    new_color = np.zeros(len(df), dtype='uint8')\n",
    "    new_color[points.point_inds] = 1\n",
    "    fig.data[1].line.color = new_color\n",
    "\n",
    "# Register callback on scatter selection...\n",
    "fig.data[0].on_selection(update_color)\n",
    "# and parcats click\n",
    "fig.data[1].on_click(update_color)\n",
    "\n",
    "# Update fig size\n",
    "fig.update_layout(width=1000, height=800)\n",
    "\n",
    "# Save as html\n",
    "# fig.write_html('FigureS3.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(f\"mydata_plotly.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variants in known disease genes: 20\n",
      "Number of variants in unknown disease genes: 20\n"
     ]
    }
   ],
   "source": [
    "df = df.loc[df['Priority Score'] >=cutoff]\n",
    "df2 = df\n",
    "df2 = df2.rename(columns={'sample': 'Case'})\n",
    "\n",
    "df2['Removed'] = df2['Case'].apply(lambda x: 'Removed' if x not in set(df['sample']) else 'Remain')\n",
    "df2 = df2.loc[df2['Removed'] == 'Remain']\n",
    "df2_known = df2.loc[df2['is_known_disease_gene'] == True].copy()\n",
    "df2_unknown = df2.loc[df2['is_known_disease_gene'] == False].copy()\n",
    "print(f\"Number of variants in known disease genes: {len(df2_known)}\")\n",
    "print(f\"Number of variants in unknown disease genes: {len(df2_unknown)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add interpriation\n",
    "interpretations: dict = {\n",
    "\t\"6-51920384-C-T\": \"Mismatched phenotype\",\n",
    "\t\"6-107050772-C-T\": \t\"Mismatched phenotype\",\n",
    "\t\"7-107414366-G-A\": \"Mismatched phenotype\",\n",
    "\t\"9-111681091-T-C\": \t\"Mismatched phenotype\",\n",
    "\t\"5-13717643-T-C\": \"Mismatched phenotype\",\n",
    "\t\"12-48387611-G-A\": \"P (Reported variant)\",\n",
    "\t\"X-153363075-G-A\": \"P (Reported variant)\",\n",
    "\t\"15-64047525-C-T\": \t\"Mismatched phenotype\",\n",
    "\t\"4-6086572-C-G\": \"LP (Novel variant)\",\n",
    "\t\"8-145138104-C-T\": \"Mismatched inheritance\",\n",
    "\t\"7-17854455-A-G\": \"Mismatched inheritance\",\n",
    "\t\"10-95262868-A-G\": \t\"Mismatched phenotype\",\n",
    "\t\"11-113196245-G-A\": \"Mismatched phenotype\",\n",
    "\t\"12-53204538-A-G\": \t\"Mismatched phenotype\",\n",
    "\t\"6-35277588-G-C\": \t\"Mismatched phenotype\",\n",
    "\t\"X-153283572-C-A\": \"Mismatched phenotype\",\n",
    "\t\"7-70246662-G-T\": \t\"Mismatched phenotype\",\n",
    "\t\"X-19373601-C-T\": \"P (Novel variant)\",\n",
    "\t\"8-8873831-G-A\": \"Mismatched phenotype\",\n",
    "\t\"10-35360126-C-T\":  \"VUS\",\n",
    "\t\"6-84624133-A-C\":   \"VUS\",\n",
    "\t\"17-45891202-T-G\":  \"VUS\",\n",
    "\t\"20-55206743-G-A\":  \"VUS\",\n",
    "\t\"2-95539264-G-A\":   \"VUS\",\n",
    "\t\"12-111311766-G-A\": \"VUS\",\n",
    "\t\"16-4918905-G-A\":   \"Novel candidate gene\",\n",
    "\t\"20-35826897-T-G\":  \"VUS\",\n",
    "\t\"8-89198703-A-T\":   \"VUS\",\n",
    "\t\"1-11129615-C-G\":   \"VUS\",\n",
    "\t\"17-46134392-A-C\":  \"Novel candidate gene\",\n",
    "\t\"1-155012960-G-A\":  \"VUS\",\n",
    "\t\"1-168211737-A-C\":  \"VUS\",\n",
    "\t\"5-141005760-C-A\":  \"VUS\",\n",
    "\t\"3-128853674-C-T\":  \"VUS\",\n",
    "\t\"14-69988999-G-T\":  \"VUS\",\n",
    "\t\"9-34618759-T-C\":   \"VUS\",\n",
    "\t\"2-39156972-G-C\":   \"VUS\",\n",
    "\t\"8-99028891-G-A\":   \"VUS\",\n",
    "\t\"11-119230372-T-C\": \"VUS\"\n",
    "}\n",
    "\n",
    "# Add inheritanace\n",
    "inheritance_list: dict = {\n",
    "\"8-145138104-C-T\": \"AR\",\n",
    "\"7-107414366-G-A\": \"AR\",\n",
    "\"7-70246662-G-T\": \"AD\",\n",
    "\"15-100794364-T-C\": \"AR\",\n",
    "\"6-107050772-C-T\": \"AR\",\n",
    "\"9-111681091-T-C\": \"AD/AR\",\n",
    "\"X-19373601-C-T\": \"XLD\",\n",
    "\"7-17854455-A-G\": \"AR\",\n",
    "\"12-48387611-G-A\": \"AD\",\n",
    "\"5-13717643-T-C\": \"AR\",\n",
    "\"10-95262868-A-G\": \"AR\",\n",
    "\"12-53204538-A-G\": \"AD\",\n",
    "\"11-113196245-G-A\": \"AR\",\n",
    "\"8-8873831-G-A\": \"AR\",\n",
    "\"6-51920384-C-T\": \"AR\",\n",
    "\"X-153363075-G-A\": \"XLD\",\n",
    "\"6-35277588-G-C\": \"AR\",\n",
    "\"4-6086572-C-G\": \"AD\",\n",
    "\"X-153283572-C-A\": \"AD\",\n",
    "\"15-64047525-C-T\": \"AR\",}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Final interpretation'] = df2['variant_id'].apply(\n",
    "\tlambda x: interpretations[x] if x in interpretations.keys() else \"VUS\")\n",
    "df2['Reported inheritance'] = df2['variant_id'].apply(\n",
    "\tlambda x: inheritance_list[x] if x in inheritance_list.keys() else \"Unknown\")\n",
    "\n",
    "df2_unkown = df2.loc[df2['is_known_disease_gene'] == False].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_known.to_excel('excel/positive_known_fixed.xlsx', index=False)\n",
    "df2_unkown.to_excel('excel/positive_unknown_fixed.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of known disease genes: 0\n",
      "Number of novel candidate genes: 2\n",
      "Number of VUS: 19\n",
      "Number of unknown disease genes: 20\n",
      "Number of phenotype mismatch: 13\n",
      "Number of LP (Novel): 1\n",
      "Number of P (Reported): 2\n",
      "Number of P (Novel): 1\n",
      "Number of inheritance mismatch: 2\n"
     ]
    }
   ],
   "source": [
    "# counting\n",
    "# known_txt = f\"Known (n = {num_known_disease_genes})\"\n",
    "# unknown_txt = f\"Unknown (n = {num_unknown_disease_genes})\"\n",
    "n_knwon = len(df2.loc[df2['Known disease gene'] == \"known\"])\n",
    "n_unknown = len(df2.loc[df2['Known disease gene'] == \"Unknown\"])\n",
    "\n",
    "n_novel = len(df2.loc[df2['Final interpretation'] == 'Novel candidate gene'])\n",
    "n_vus = len(df2.loc[df2['Final interpretation'] == 'VUS'])\n",
    "n_phenomismatch = len(df2.loc[df2['Final interpretation'] == 'Mismatched phenotype'])\n",
    "n_lp = len(df2.loc[df2['Final interpretation'] == 'LP (Novel variant)'])\n",
    "n_p_reported = len(df2.loc[df2['Final interpretation'] == 'P (Reported variant)'])\n",
    "n_p_novel = len(df2.loc[df2['Final interpretation'] == 'P (Novel variant)'])\n",
    "n_inhmismatch = len(df2.loc[df2['Final interpretation'] == 'Mismatched inheritance'])\n",
    "\n",
    "print(f\"Number of known disease genes: {n_knwon}\")\n",
    "print(f\"Number of novel candidate genes: {n_novel}\")\n",
    "print(f\"Number of VUS: {n_vus}\")\n",
    "print(f\"Number of unknown disease genes: {n_unknown}\")\n",
    "print(f\"Number of phenotype mismatch: {n_phenomismatch}\")\n",
    "print(f\"Number of LP (Novel): {n_lp}\")\n",
    "print(f\"Number of P (Reported): {n_p_reported}\")\n",
    "print(f\"Number of P (Novel): {n_p_novel}\")\n",
    "print(f\"Number of inheritance mismatch: {n_inhmismatch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_num_to_gene_class(x) -> str:\n",
    "    if x == 'Known':\n",
    "        return f\"Known <br>(n = {n_knwon})\"\n",
    "    else:\n",
    "        return f\"Unknown <br>(n = {n_unknown})\"\n",
    "\n",
    "def add_num_to_interpretation(x) -> str:\n",
    "    if x == 'Novel candidate gene':\n",
    "        return f\"Novel candidate gene (n = {n_novel})\"\n",
    "    elif x == 'VUS':\n",
    "        return f\"VUS (n = {n_vus})\"\n",
    "    elif x == 'Phenotype mismatch':\n",
    "        return f\"Mismatched phenotype (n = {n_phenomismatch})\"\n",
    "    elif x == 'LP (Novel)':\n",
    "        return f\"LP (Novel) (n = {n_lp})\"\n",
    "    elif x == 'P (Reported)':\n",
    "        return f\"P (Reported) (n = {n_p_reported})\"\n",
    "    elif x == 'P (Novel)':\n",
    "        return f\"P (Novel) (n = {n_p_novel})\"\n",
    "    elif x == 'Inheritance mismatch':\n",
    "        return f\"Mismatched inheritance (n = {n_inhmismatch})\"\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "# df2['Known disease gene'] = df2['Known disease gene'].apply(add_num_to_gene_class)\n",
    "df2['Final interpretation'] = df2['Final interpretation'].apply(add_num_to_interpretation)    \n",
    "\n",
    "categorical_dimensions = [\"Known disease gene\", \"Reported inheritance\", \"eLoF\", \"Priority Score\", \"Final interpretation\"]\n",
    "dimensions = [dict(values=df2[label], label=label) for label in categorical_dimensions]\n",
    "\n",
    "# Build colorscale\n",
    "color = np.zeros(len(df), dtype='uint8')\n",
    "colorscale = [[0, 'lightgray'], [1, 'red']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd118032b33e40718914453d803f2a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(ToggleButtons(description='Brush Color:', index=1, options=('None', 'Red', 'Blue', 'Orange', 'G…"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build colorscale\n",
    "color = np.zeros(len(df2), dtype='uint8')\n",
    "colorscale = [[0, 'gray'], [0.14, 'gray'],                  # Gray\n",
    "              [0.14, 'firebrick'], [0.28, 'firebrick'],     # Red\n",
    "              [0.28, '#82C6EF'], [0.42, '#82C6EF'],          # Blue\n",
    "              [0.42, '#EAA63A'], [0.56, '#EAA63A'],          # Orange\n",
    "              [0.56, '#4FAC7E'], [0.70, '#4FAC7E'],          # Green\n",
    "              [0.70, '#E3948D'], [0.84, '#E3948D'],         # Pink\n",
    "              [0.84, '#804031'], [1, '#804031']]            # Brown\n",
    "cmin, cmax = 0, 7\n",
    "\n",
    "# cmin, cmax = 0, 1\n",
    "fig = go.FigureWidget(\n",
    "    data=[\n",
    "        go.Scatter(\n",
    "            x=df2['Final interpretation'], \n",
    "            y=df2['Priority Score'],\n",
    "            marker={\n",
    "                'color': color, 'cmin': cmin, 'cmax': cmax,\n",
    "                'colorscale': colorscale, 'showscale': False,\n",
    "                'colorbar': {\n",
    "                    'tickvals': [0, 1, 2, 3, 4, 5, 6], \n",
    "                    'ticktext': ['None', 'Red', 'Blue', 'Pink', 'Orange', 'Green', 'Yellow']\n",
    "                    }\n",
    "                },\n",
    "            # Edit size and line\n",
    "            \n",
    "            mode='markers'),\n",
    "        go.Parcats(\n",
    "            domain={'y': [0, 0.4]}, dimensions=dimensions,\n",
    "            line={'colorscale': colorscale, 'cmin': cmin,\n",
    "                   'cmax': cmax, 'color': color, 'shape': 'hspline'})]\n",
    ")\n",
    "\n",
    "x_labels = ['Novel<br>candidate gene', 'VUS', 'P<br>(Reported)', 'P<br>(Novel)', \n",
    "            'LP<br>(Novel)', 'Mismatched<br>phenotype', 'Mismatched<br>inhritance']\n",
    "\n",
    "fig.update_layout(height=600, width=800,\n",
    "                  xaxis={\n",
    "                      'title': 'Final Interpretation', \n",
    "                      'tickvals': [0, 1, 2, 3, 4, 5, 6],\n",
    "                      'ticktext': x_labels\n",
    "                      },\n",
    "                  yaxis={'title': 'Priority Score', 'domain': [0.6, 1]},\n",
    "                  dragmode='lasso', hovermode='closest')\n",
    "\n",
    "# fig.update_xaxes(categoryorder='array', categoryarray=x_labels)\n",
    "\n",
    "# Build color selection widget\n",
    "color_toggle = widgets.ToggleButtons(\n",
    "    options=['None', 'Red', 'Blue', 'Orange', 'Green', 'Pink', 'Brown'],\n",
    "    style={'button_width': '96px', 'font_size': '14px'},\n",
    "    index=1, description='Brush Color:', disabled=False)\n",
    "\n",
    "# Update color callback\n",
    "def update_color(trace, points, state):\n",
    "    # Compute new color array\n",
    "    new_color = np.array(fig.data[0].marker.color)\n",
    "    new_color[points.point_inds] = color_toggle.index\n",
    "\n",
    "    with fig.batch_update():\n",
    "        # Update scatter color\n",
    "        fig.data[0].marker.color = new_color\n",
    "\n",
    "        # Update parcats colors\n",
    "        fig.data[1].line.color = new_color\n",
    "\n",
    "# Register callback on scatter selection...\n",
    "fig.data[0].on_selection(update_color)\n",
    "# and parcats click\n",
    "fig.data[1].on_click(update_color)\n",
    "fig.update_layout(margin=dict(t=20, b=20, l=10, r=120))\n",
    "# Update fig size\n",
    "fig.update_layout(width=800, height=1000)\n",
    "\n",
    "# Display figure\n",
    "widgets.VBox([color_toggle, fig])\n",
    "\n",
    "\n",
    "# Save as html\n",
    "# fig.write_html('FigureS4.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
